The following text is a Git repository with code. The structure of the text are sections that begin with ----, followed by a single line containing the file path and file name, followed by a variable amount of lines containing the file contents. The text representing the Git repository ends when the symbols --END-- are encounted. Any further text beyond --END-- are meant to be interpreted as instructions using the aforementioned Git repository as context.
----
README.md
# Sketch Colorization Using Diffusion Models & Photo-Sketch Correspondence

[![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/AxelDlv00/DiffusionSketchColorization) [![Hugging Face Dataset](https://img.shields.io/badge/HuggingFace-Dataset-yellow?logo=huggingface)](https://huggingface.co/datasets/ComputerVisionAnimeProject/AnimeFaceColorization/blob/main/README.md) [![AniDiffusion Model](https://img.shields.io/badge/HuggingFace-Model-blue?logo=huggingface)](https://huggingface.co/ComputerVisionAnimeProject/AniDiffusionModel)

## Overview
This project explores **anime sketch colorization** using state-of-the-art **diffusion models** and **photo-sketch correspondence techniques**. Inspired by recent advancements in **AnimeDiffusion**, **MangaNinja**, and **photo-sketch correspondence models**, our method is a lighter model.

**Read the Full Paper:** [Sketch Colorization Using Diffusion Models](./sketch_colorization.pdf)

## Datasets Used
We created a **curated dataset** combining:
- **AnimeFace** [[Dataset](https://www.kaggle.com/datasets/thedevastator/anime-face-dataset-by-character-name)]
- **Danbooru Tagged Dataset** [[Dataset](https://www.kaggle.com/datasets/mylesoneill/tagged-anime-illustrations)]
- **[AnimeDiffusion Dataset](https://xq-meng.github.io/projects/AnimeDiffusion/)** augmented with **deformation flows**  

## Getting Started
### Installation
To set up the environment, install dependencies using:

```bash
git clone https://github.com/AxelDlv00/DiffusionSketchColorization.git
cd DiffusionSketchColorization
```

## Citing
If you use this model, please cite:
```
@misc{delavalkoita2025sketchcolorization,
  author       = {Axel Delaval and Adama Koïta},
  title        = {Sketch Colorization Using Diffusion Models and Photo-Sketch Correspondence},
  year         = {2025},
  url          = {https://github.com/AxelDlv00/DiffusionSketchColorization},
  note         = {Project exploring anime sketch colorization using diffusion models and deep learning.}
}
```

----
distributed.py
"""
===========================================================
Distributed Initialization Script
===========================================================

This script initializes the distributed process group for training models using multiple GPUs. It includes the following steps:
1. Setting the device for the current process
2. Initializing the process group using the NCCL backend

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import os
import torch
import torch.distributed as dist

def init_distributed():
    """
    Initialize the distributed process group.
    Environment variables (RANK, WORLD_SIZE, LOCAL_RANK) must be set.

    Returns:
        rank (int): Rank of the current process.
        world_size (int): Total number of processes.
        local_rank (int): Local rank of the current process.
    """
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl", init_method="env://")
    return rank, world_size, local_rank

----
trainer.py
"""
===========================================================
Trainer Script for Sketch Colorization Using Diffusion Models
===========================================================

This script handles the training process for the sketch colorization model using diffusion models and photo-sketch correspondence techniques. It includes the following steps:
1. Distributed initialization
2. Loading the PSC model
3. Loading the dataset
4. Initializing the models
5. Wrapping models in DistributedDataParallel (DDP)
6. Setting up the optimizer and scheduler
7. Training loop with checkpointing

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import os
import json
import torch
import torch.optim as optim
from torch.utils.data import DistributedSampler, DataLoader
from tqdm import tqdm
from functools import partial

from models.reference_unet import ReferenceUNet
from models.denoising_unet import DenoisingUNet
from models.psc_diffusion import PSCGaussianDiffusion
from psc_project.PSCNet import PSCNet
from psc_project.moco import MoCo
from psc_project.resnet_cbn import resnet101
from utils.data.dataset_utils import CustomAnimeDataset, worker_init_fn
from train.distributed import init_distributed

def save_loss_history(loss_list, checkpoint_dir):
    """Save the loss history to a JSON file."""
    loss_file = os.path.join(checkpoint_dir, "loss_history.json")
    with open(loss_file, "w") as f:
        json.dump(loss_list, f, indent=4)

def load_loss_history(checkpoint_dir):
    """Load the loss history from a JSON file."""
    loss_file = os.path.join(checkpoint_dir, "loss_history.json")
    if os.path.exists(loss_file):
        with open(loss_file, "r") as f:
            return json.load(f)
    return []

def custom_train(dataset_path,
                 epochs,
                 batch_size,
                 patch_size,
                 lr,
                 train_ratio,
                 subset_percentage,
                 psc_checkpoint_path,
                 checkpoint_dir,
                 N_refresh_log,
                 image_size):
    """
    Custom training function for the sketch colorization model.

    Args:
        dataset_path (str): Path to the dataset.
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.
        patch_size (int): Size of the patches.
        lr (float): Learning rate.
        train_ratio (float): Ratio of training data.
        subset_percentage (float): Percentage of the dataset to use.
        psc_checkpoint_path (str): Path to the PSC model checkpoint.
        checkpoint_dir (str): Directory to save checkpoints.
        N_refresh_log (int): Frequency of logging and checkpointing.
        image_size (int): Size of the input images.

    Returns:
        ref_unet, denoising_unet, gaussian_diffusion, PSC_MODEL_WORKER, Loss
    """
    # Distributed initialization
    rank, world_size, local_rank = init_distributed()
    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(device)

    # 1. Load the main PSC model onto the correct device
    PSC_MODEL_WORKER = PSCNet(MoCo, resnet101, dim=128, K=8192, corr_layer=[2, 3]).to(device)
    if os.path.isfile(psc_checkpoint_path):
        checkpoint_psc = torch.load(psc_checkpoint_path, map_location=device)
        state_dict = checkpoint_psc["state_dict"]
        for k in list(state_dict.keys()):
            if k.startswith("module."):
                state_dict[k[len("module."):]] = state_dict.pop(k)
        PSC_MODEL_WORKER.load_state_dict(state_dict, strict=False)
    else:
        if rank == 0:
            print(f"[Warning] PSC checkpoint not found at {psc_checkpoint_path} - continuing anyway")

    if rank == 0:
        os.makedirs(checkpoint_dir, exist_ok=True)
    last_checkpoint_file = os.path.join(checkpoint_dir, "last_checkpoint.pth.tar")
    best_checkpoint_file = os.path.join(checkpoint_dir, "best_checkpoint.pth.tar")

    # 2. Load the dataset
    sketch_dir = os.path.join(dataset_path, "sketch")
    reference_dir = os.path.join(dataset_path, "reference")
    train_dataset = CustomAnimeDataset(None, sketch_dir, reference_dir, subset_percentage=subset_percentage)
    train_dataset.set_psc_model(PSC_MODEL_WORKER)
    if rank == 0:
        print(f"Dataset size: {len(train_dataset)} total triplets.")

    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    worker_fn = partial(worker_init_fn, local_rank=local_rank)
    n_gpu = torch.cuda.device_count()

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,
                              num_workers=0, pin_memory=False, worker_init_fn=worker_fn)

    # 3. Initialize the other models
    ref_unet = ReferenceUNet(in_channels=3, base_ch=64, num_res_blocks=3, num_attn_blocks=3, cbam=False).to(device)
    denoising_unet = DenoisingUNet(channel_in=9, channel_out=3, channel_base=64, channel_features=64,
                                   n_res_blocks=2, dropout=0.1, channel_mult=(1,2,4,8),
                                   attention_head=4, cbam=True).to(device)
    gaussian_diffusion = PSCGaussianDiffusion(time_step=1000,
                                              betas={"linear_start": 0.0001, "linear_end": 0.02},
                                              denoising_unet=denoising_unet,
                                              psc_model=PSC_MODEL_WORKER).to(device)

    # 4. Wrap models in DDP with find_unused_parameters=True
    from torch.nn.parallel import DistributedDataParallel as DDP
    ref_unet = DDP(ref_unet, device_ids=[local_rank], find_unused_parameters=True)
    denoising_unet = DDP(denoising_unet, device_ids=[local_rank], find_unused_parameters=True)
    gaussian_diffusion = DDP(gaussian_diffusion, device_ids=[local_rank], find_unused_parameters=True)
    PSC_MODEL_WORKER = DDP(PSC_MODEL_WORKER, device_ids=[local_rank], find_unused_parameters=True)

    # 5. Optimizer and scheduler
    optimizer = optim.AdamW(list(ref_unet.parameters()) +
                            list(denoising_unet.parameters()) +
                            list(gaussian_diffusion.parameters()), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

    Loss = load_loss_history(checkpoint_dir)
    start_epoch = 0
    if os.path.exists(last_checkpoint_file) and rank == 0:
        checkpoint = torch.load(last_checkpoint_file, map_location=device)
        ref_unet.module.load_state_dict(checkpoint["ref_unet_state_dict"])
        denoising_unet.module.load_state_dict(checkpoint["denoising_unet_state_dict"])
        gaussian_diffusion.module.load_state_dict(checkpoint["gaussian_diffusion_state_dict"])
        PSC_MODEL_WORKER.module.load_state_dict(checkpoint["psc_model_state_dict"])

        try:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        except ValueError:
            print("[WARNING] Optimizer state does not match, reinitializing optimizer.")
            optimizer = optim.AdamW(set(ref_unet.parameters()) | set(denoising_unet.parameters()) | set(gaussian_diffusion.parameters()), lr=lr)

        start_epoch = checkpoint["epoch"] + 1
        print(f"Resumed at epoch {start_epoch}")
    start_epoch_tensor = torch.tensor(start_epoch, device=device)
    torch.distributed.broadcast(start_epoch_tensor, src=0)
    start_epoch = int(start_epoch_tensor.item())

    if rank == 0:
        print(f"Starting training on {len(train_dataset)} samples for {epochs} epochs...")

    ref_unet.train()
    denoising_unet.train()
    gaussian_diffusion.train()
    PSC_MODEL_WORKER.eval()  # PSC remains in evaluation mode

    try:
        for epoch in range(start_epoch, epochs):
            train_sampler.set_epoch(epoch)
            total_loss = 0.0

            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}", disable=(rank != 0)), 1):
                sketch, image, other_image, patched_ref, flow_warp = [x.to(device) for x in batch]
                t = torch.randint(0, 1000, (sketch.shape[0],), device=device).long()

                ref_feats = ref_unet(patched_ref)
                loss = gaussian_diffusion(image, t, sketch, flow_warp, ref_feats)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(train_loader)
            Loss.append(avg_loss)
            scheduler.step()

            if rank == 0:
                print(f"Epoch [{epoch+1}/{epochs}] average loss: {avg_loss:.4f}")
                save_loss_history(Loss, checkpoint_dir)

                checkpoint_dict = {
                    "epoch": epoch,
                    "ref_unet_state_dict": ref_unet.module.state_dict(),
                    "denoising_unet_state_dict": denoising_unet.module.state_dict(),
                    "gaussian_diffusion_state_dict": gaussian_diffusion.module.state_dict(),
                    "psc_model_state_dict": PSC_MODEL_WORKER.module.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict()
                }
                torch.save(checkpoint_dict, last_checkpoint_file)
                if avg_loss == min(Loss):
                    print("[INFO] Saving new best checkpoint...")
                    torch.save(checkpoint_dict, best_checkpoint_file)

                if (epoch + 1) % N_refresh_log == 0:
                    torch.save(checkpoint_dict, os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth.tar"))
    except KeyboardInterrupt:
        if rank == 0:
            print("[INFO] Training interrupted. Saving loss history before exiting...")
            save_loss_history(Loss, checkpoint_dir)
    finally:
        torch.distributed.destroy_process_group()

    return ref_unet, denoising_unet, gaussian_diffusion, PSC_MODEL_WORKER, Loss

"""
import json
import matplotlib.pyplot as plt

loss_file = "path/to/checkpoint_dir/loss_history.json"
with open(loss_file, "r") as f:
    loss_data = json.load(f)

plt.plot(loss_data)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Evolution During Training")
plt.show()
"""
----
output.txt
The following text is a Git repository with code. The structure of the text are sections that begin with ----, followed by a single line containing the file path and file name, followed by a variable amount of lines containing the file contents. The text representing the Git repository ends when the symbols --END-- are encounted. Any further text beyond --END-- are meant to be interpreted as instructions using the aforementioned Git repository as context.
----
README.md
# Sketch Colorization Using Diffusion Models & Photo-Sketch Correspondence

[![GitHub Repository](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/AxelDlv00/DiffusionSketchColorization) [![Hugging Face Dataset](https://img.shields.io/badge/HuggingFace-Dataset-yellow?logo=huggingface)](https://huggingface.co/datasets/ComputerVisionAnimeProject/AnimeFaceColorization/blob/main/README.md) [![AniDiffusion Model](https://img.shields.io/badge/HuggingFace-Model-blue?logo=huggingface)](https://huggingface.co/ComputerVisionAnimeProject/AniDiffusionModel)

## Overview
This project explores **anime sketch colorization** using state-of-the-art **diffusion models** and **photo-sketch correspondence techniques**. Inspired by recent advancements in **AnimeDiffusion**, **MangaNinja**, and **photo-sketch correspondence models**, our method is a lighter model.

**Read the Full Paper:** [Sketch Colorization Using Diffusion Models](./sketch_colorization.pdf)

## Datasets Used
We created a **curated dataset** combining:
- **AnimeFace** [[Dataset](https://www.kaggle.com/datasets/thedevastator/anime-face-dataset-by-character-name)]
- **Danbooru Tagged Dataset** [[Dataset](https://www.kaggle.com/datasets/mylesoneill/tagged-anime-illustrations)]
- **[AnimeDiffusion Dataset](https://xq-meng.github.io/projects/AnimeDiffusion/)** augmented with **deformation flows**  

## Getting Started
### Installation
To set up the environment, install dependencies using:

```bash
git clone https://github.com/AxelDlv00/DiffusionSketchColorization.git
cd DiffusionSketchColorization
```

## Citing
If you use this model, please cite:
```
@misc{delavalkoita2025sketchcolorization,
  author       = {Axel Delaval and Adama Koïta},
  title        = {Sketch Colorization Using Diffusion Models and Photo-Sketch Correspondence},
  year         = {2025},
  url          = {https://github.com/AxelDlv00/DiffusionSketchColorization},
  note         = {Project exploring anime sketch colorization using diffusion models and deep learning.}
}
```

----
distributed.py
"""
===========================================================
Distributed Initialization Script
===========================================================

This script initializes the distributed process group for training models using multiple GPUs. It includes the following steps:
1. Setting the device for the current process
2. Initializing the process group using the NCCL backend

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import os
import torch
import torch.distributed as dist

def init_distributed():
    """
    Initialize the distributed process group.
    Environment variables (RANK, WORLD_SIZE, LOCAL_RANK) must be set.

    Returns:
        rank (int): Rank of the current process.
        world_size (int): Total number of processes.
        local_rank (int): Local rank of the current process.
    """
    rank = int(os.environ.get("RANK", 0))
    world_size = int(os.environ.get("WORLD_SIZE", 1))
    local_rank = int(os.environ.get("LOCAL_RANK", 0))
    torch.cuda.set_device(local_rank)
    dist.init_process_group(backend="nccl", init_method="env://")
    return rank, world_size, local_rank

----
trainer.py
"""
===========================================================
Trainer Script for Sketch Colorization Using Diffusion Models
===========================================================

This script handles the training process for the sketch colorization model using diffusion models and photo-sketch correspondence techniques. It includes the following steps:
1. Distributed initialization
2. Loading the PSC model
3. Loading the dataset
4. Initializing the models
5. Wrapping models in DistributedDataParallel (DDP)
6. Setting up the optimizer and scheduler
7. Training loop with checkpointing

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import os
import json
import torch
import torch.optim as optim
from torch.utils.data import DistributedSampler, DataLoader
from tqdm import tqdm
from functools import partial

from models.reference_unet import ReferenceUNet
from models.denoising_unet import DenoisingUNet
from models.psc_diffusion import PSCGaussianDiffusion
from psc_project.PSCNet import PSCNet
from psc_project.moco import MoCo
from psc_project.resnet_cbn import resnet101
from utils.data.dataset_utils import CustomAnimeDataset, worker_init_fn
from train.distributed import init_distributed

def save_loss_history(loss_list, checkpoint_dir):
    """Save the loss history to a JSON file."""
    loss_file = os.path.join(checkpoint_dir, "loss_history.json")
    with open(loss_file, "w") as f:
        json.dump(loss_list, f, indent=4)

def load_loss_history(checkpoint_dir):
    """Load the loss history from a JSON file."""
    loss_file = os.path.join(checkpoint_dir, "loss_history.json")
    if os.path.exists(loss_file):
        with open(loss_file, "r") as f:
            return json.load(f)
    return []

def custom_train(dataset_path,
                 epochs,
                 batch_size,
                 patch_size,
                 lr,
                 train_ratio,
                 subset_percentage,
                 psc_checkpoint_path,
                 checkpoint_dir,
                 N_refresh_log,
                 image_size):
    """
    Custom training function for the sketch colorization model.

    Args:
        dataset_path (str): Path to the dataset.
        epochs (int): Number of training epochs.
        batch_size (int): Batch size for training.
        patch_size (int): Size of the patches.
        lr (float): Learning rate.
        train_ratio (float): Ratio of training data.
        subset_percentage (float): Percentage of the dataset to use.
        psc_checkpoint_path (str): Path to the PSC model checkpoint.
        checkpoint_dir (str): Directory to save checkpoints.
        N_refresh_log (int): Frequency of logging and checkpointing.
        image_size (int): Size of the input images.

    Returns:
        ref_unet, denoising_unet, gaussian_diffusion, PSC_MODEL_WORKER, Loss
    """
    # Distributed initialization
    rank, world_size, local_rank = init_distributed()
    device = torch.device(f"cuda:{local_rank}")
    torch.cuda.set_device(device)

    # 1. Load the main PSC model onto the correct device
    PSC_MODEL_WORKER = PSCNet(MoCo, resnet101, dim=128, K=8192, corr_layer=[2, 3]).to(device)
    if os.path.isfile(psc_checkpoint_path):
        checkpoint_psc = torch.load(psc_checkpoint_path, map_location=device)
        state_dict = checkpoint_psc["state_dict"]
        for k in list(state_dict.keys()):
            if k.startswith("module."):
                state_dict[k[len("module."):]] = state_dict.pop(k)
        PSC_MODEL_WORKER.load_state_dict(state_dict, strict=False)
    else:
        if rank == 0:
            print(f"[Warning] PSC checkpoint not found at {psc_checkpoint_path} - continuing anyway")

    if rank == 0:
        os.makedirs(checkpoint_dir, exist_ok=True)
    last_checkpoint_file = os.path.join(checkpoint_dir, "last_checkpoint.pth.tar")
    best_checkpoint_file = os.path.join(checkpoint_dir, "best_checkpoint.pth.tar")

    # 2. Load the dataset
    sketch_dir = os.path.join(dataset_path, "sketch")
    reference_dir = os.path.join(dataset_path, "reference")
    train_dataset = CustomAnimeDataset(None, sketch_dir, reference_dir, subset_percentage=subset_percentage)
    train_dataset.set_psc_model(PSC_MODEL_WORKER)
    if rank == 0:
        print(f"Dataset size: {len(train_dataset)} total triplets.")

    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)
    worker_fn = partial(worker_init_fn, local_rank=local_rank)
    n_gpu = torch.cuda.device_count()

    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler,
                              num_workers=0, pin_memory=False, worker_init_fn=worker_fn)

    # 3. Initialize the other models
    ref_unet = ReferenceUNet(in_channels=3, base_ch=64, num_res_blocks=3, num_attn_blocks=3, cbam=False).to(device)
    denoising_unet = DenoisingUNet(channel_in=9, channel_out=3, channel_base=64, channel_features=64,
                                   n_res_blocks=2, dropout=0.1, channel_mult=(1,2,4,8),
                                   attention_head=4, cbam=True).to(device)
    gaussian_diffusion = PSCGaussianDiffusion(time_step=1000,
                                              betas={"linear_start": 0.0001, "linear_end": 0.02},
                                              denoising_unet=denoising_unet,
                                              psc_model=PSC_MODEL_WORKER).to(device)

    # 4. Wrap models in DDP with find_unused_parameters=True
    from torch.nn.parallel import DistributedDataParallel as DDP
    ref_unet = DDP(ref_unet, device_ids=[local_rank], find_unused_parameters=True)
    denoising_unet = DDP(denoising_unet, device_ids=[local_rank], find_unused_parameters=True)
    gaussian_diffusion = DDP(gaussian_diffusion, device_ids=[local_rank], find_unused_parameters=True)
    PSC_MODEL_WORKER = DDP(PSC_MODEL_WORKER, device_ids=[local_rank], find_unused_parameters=True)

    # 5. Optimizer and scheduler
    optimizer = optim.AdamW(list(ref_unet.parameters()) +
                            list(denoising_unet.parameters()) +
                            list(gaussian_diffusion.parameters()), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)

    Loss = load_loss_history(checkpoint_dir)
    start_epoch = 0
    if os.path.exists(last_checkpoint_file) and rank == 0:
        checkpoint = torch.load(last_checkpoint_file, map_location=device)
        ref_unet.module.load_state_dict(checkpoint["ref_unet_state_dict"])
        denoising_unet.module.load_state_dict(checkpoint["denoising_unet_state_dict"])
        gaussian_diffusion.module.load_state_dict(checkpoint["gaussian_diffusion_state_dict"])
        PSC_MODEL_WORKER.module.load_state_dict(checkpoint["psc_model_state_dict"])

        try:
            optimizer.load_state_dict(checkpoint["optimizer_state_dict"])
        except ValueError:
            print("[WARNING] Optimizer state does not match, reinitializing optimizer.")
            optimizer = optim.AdamW(set(ref_unet.parameters()) | set(denoising_unet.parameters()) | set(gaussian_diffusion.parameters()), lr=lr)

        start_epoch = checkpoint["epoch"] + 1
        print(f"Resumed at epoch {start_epoch}")
    start_epoch_tensor = torch.tensor(start_epoch, device=device)
    torch.distributed.broadcast(start_epoch_tensor, src=0)
    start_epoch = int(start_epoch_tensor.item())

    if rank == 0:
        print(f"Starting training on {len(train_dataset)} samples for {epochs} epochs...")

    ref_unet.train()
    denoising_unet.train()
    gaussian_diffusion.train()
    PSC_MODEL_WORKER.eval()  # PSC remains in evaluation mode

    try:
        for epoch in range(start_epoch, epochs):
            train_sampler.set_epoch(epoch)
            total_loss = 0.0

            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f"Epoch {epoch+1}", disable=(rank != 0)), 1):
                sketch, image, other_image, patched_ref, flow_warp = [x.to(device) for x in batch]
                t = torch.randint(0, 1000, (sketch.shape[0],), device=device).long()

                ref_feats = ref_unet(patched_ref)
                loss = gaussian_diffusion(image, t, sketch, flow_warp, ref_feats)

                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                total_loss += loss.item()

            avg_loss = total_loss / len(train_loader)
            Loss.append(avg_loss)
            scheduler.step()

            if rank == 0:
                print(f"Epoch [{epoch+1}/{epochs}] average loss: {avg_loss:.4f}")
                save_loss_history(Loss, checkpoint_dir)

                checkpoint_dict = {
                    "epoch": epoch,
                    "ref_unet_state_dict": ref_unet.module.state_dict(),
                    "denoising_unet_state_dict": denoising_unet.module.state_dict(),
                    "gaussian_diffusion_state_dict": gaussian_diffusion.module.state_dict(),
                    "psc_model_state_dict": PSC_MODEL_WORKER.module.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict()
                }
                torch.save(checkpoint_dict, last_checkpoint_file)
                if avg_loss == min(Loss):
                    print("[INFO] Saving new best checkpoint...")
                    torch.save(checkpoint_dict, best_checkpoint_file)

                if (epoch + 1) % N_refresh_log == 0:
                    torch.save(checkpoint_dict, os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch+1}.pth.tar"))
    except KeyboardInterrupt:
        if rank == 0:
            print("[INFO] Training interrupted. Saving loss history before exiting...")
            save_loss_history(Loss, checkpoint_dir)
    finally:
        torch.distributed.destroy_process_group()

    return ref_unet, denoising_unet, gaussian_diffusion, PSC_MODEL_WORKER, Loss

"""
import json
import matplotlib.pyplot as plt

loss_file = "path/to/checkpoint_dir/loss_history.json"
with open(loss_file, "r") as f:
    loss_data = json.load(f)

plt.plot(loss_data)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Loss Evolution During Training")
plt.show()
"""

----
utils/__init__.py
from .logger import Logger
----
utils/visualization/visualize_sample_loader.py
"""
===========================================================
PSCNet Model Visualization Utilities
===========================================================

This script contains utility functions for visualizing the outputs of the PSCNet model. It includes the following functions:
1. visualize_samples_loader: Visualize samples from the test DataLoader by generating colorized outputs using the provided models.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import random
import matplotlib.pyplot as plt
import torchvision.transforms.functional as TF
from utils.image.warp_psc import warp_reference_with_psc

@torch.no_grad()
def visualize_samples_loader(
    test_loader,
    diffusion_model,  # PSCGaussianDiffusion
    ref_unet,
    psc_model,
    device="cuda",
    num_samples=5
):
    """
    Visualize samples from the test DataLoader by generating colorized outputs
    using PSCGaussianDiffusion.

    We assume each batch from test_loader is:
      (sketch, image, other_image, patched_ref, flow_warp)
    and each is shape [1, 3, H, W] if batch_size=1.
    """
    device = torch.device(device if torch.cuda.is_available() else "cpu")

    # We only want to visualize up to `num_samples` batches
    # Convert the loader to an iterator
    test_loader_iter = iter(test_loader)

    for _ in range(num_samples):
        try:
            batch = next(test_loader_iter)
        except StopIteration:
            break

        # batch is a tuple of length 5 (each shape [1, 3, H, W] if batch_size=1)
        (
            sketch_of_the_character,
            image_of_the_character,
            other_image_of_the_character,
            patched_ref,
            flow_warp
        ) = [x.to(device) for x in batch]

        # Shapes now (assuming batch_size=1):
        #  sketch_of_the_character: [1, 3, H, W]
        #  image_of_the_character : [1, 3, H, W]
        #  other_image_of_the_character: [1, 3, H, W]
        #  patched_ref: [1, 3, H, W]
        #  flow_warp : [1, 3, H, W]

        # 1) Get reference feats from 'patched_ref'
        ref_feats = ref_unet(patched_ref)  # up to your model details

        # 2) Create random noise x_T of shape [1, 3, H, W]
        x_t = torch.randn_like(other_image_of_the_character)

        # 3) Build the 6-channel conditioning: [1, 6, H, W]
        #    (sketch + flow_warp)
        x_cond = torch.cat([sketch_of_the_character, flow_warp], dim=1)

        # 4) Run the full reverse diffusion
        output_list = diffusion_model.inference(
            x_t=x_t,
            ref_feats=ref_feats,
            x_cond=x_cond,
            eta=1.0
        )
        final_output = output_list[-1]  # The final image

        # Convert each tensor to a NumPy image for plotting
        sketch_np = sketch_of_the_character[0].cpu().permute(1, 2, 0).numpy()  # [H,W,3]
        flow_np   = flow_warp[0].cpu().permute(1, 2, 0).numpy()                # [H,W,3]
        output_np = final_output[0].cpu().permute(1,2,0).numpy()               # [H,W,3]
        gt_np     = image_of_the_character[0].cpu().permute(1, 2, 0).numpy()   # [H,W,3]
        other_np  = other_image_of_the_character[0].cpu().permute(1, 2, 0).numpy()

        # Plot
        fig, axes = plt.subplots(2, 3, figsize=(15, 10))

        axes[0, 0].imshow(sketch_np)
        axes[0, 0].set_title("Sketch")

        axes[0, 1].imshow(other_np)
        axes[0, 1].set_title("Other Ref (Unwarped)")

        axes[0, 2].imshow(flow_np)
        axes[0, 2].set_title("Flow Warp")

        axes[1, 0].imshow(gt_np)
        axes[1, 0].set_title("Ground Truth Image")

        axes[1, 1].imshow(output_np)
        axes[1, 1].set_title("Diffusion Output")

        axes[1, 2].axis('off')
        plt.tight_layout()
        plt.show()

----
utils/visualization/psc_model_plot.py
"""
===========================================================
PSCNet Model Visualization Utilities
===========================================================

This script contains utility functions for visualizing the outputs of the PSCNet model. It includes the following functions:
1. visualize_samples: Visualize samples from the test dataset by generating colorized outputs using the provided models.
2. visualize_samples_loader: Visualize samples from the test DataLoader by generating colorized outputs using the provided models.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
from utils.image.warp_psc import warp_reference_with_psc
import matplotlib.pyplot as plt
import random

def visualize_samples(test_dataset, ref_unet, denoise_unet, psc_model, device="cuda", num_samples=5):
    """
    Visualize samples from the test dataset by generating colorized outputs using the provided models.

    Args:
        test_dataset (Dataset): The dataset containing test samples.
        ref_unet (nn.Module): The ReferenceUNet model used for extracting features from the reference image.
        denoise_unet (nn.Module): The DenoisingUNet model used for generating the final colorized output.
        psc_model (nn.Module): The PSCNet model used for warping the reference image.
        device (str): The device to run the models on ("cuda" for GPU, "cpu" for CPU).
        num_samples (int): The number of samples to visualize.

    Returns:
        None
    """
    # Set the device to GPU if available, otherwise use CPU
    device = torch.device(device if torch.cuda.is_available() else "cpu")
    dataset_size = len(test_dataset)
    num_samples = min(num_samples, dataset_size)
    # Shuffle indices to randomly select samples
    random_indices = random.sample(range(dataset_size), num_samples)
    
    for i in random_indices:
        # Get a sketch sample from the dataset
        sketch, _, _ = test_dataset[i]
        # Randomly choose a mismatched reference sample
        j = random.randint(0, dataset_size - 1)
        _, mismatched_reference, mismatched_patched_ref = test_dataset[j]
        # Add batch dimension and move to the selected device
        sketch = sketch.unsqueeze(0).to(device)
        mismatched_reference = mismatched_reference.unsqueeze(0).to(device)
        mismatched_patched_ref = mismatched_patched_ref.unsqueeze(0).to(device)
        # Create a noisy sketch: convert sketch to grayscale and add Gaussian noise
        sketch_gray = sketch.mean(dim=1, keepdim=True)
        noisy_sketch = torch.clamp(sketch_gray + 0.1 * torch.randn_like(sketch_gray), 0., 1.)
        # Warp the mismatched reference using PSCNet (using the sketch and mismatched reference)
        warped_ref = warp_reference_with_psc(sketch, mismatched_reference, psc_model)
        # Concatenate the noisy sketch and the warped mismatched reference
        denoise_input = torch.cat([noisy_sketch, warped_ref], dim=1)
        # Extract features from the mismatched patched reference using ReferenceUNet
        ref_features = ref_unet(mismatched_patched_ref)
        # Generate the output using the DenoisingUNet
        output = denoise_unet(denoise_input, ref_features)
        # Convert tensors to numpy for visualization
        noisy_np = noisy_sketch[0].detach().cpu().squeeze().numpy()
        warped_np = warped_ref[0].detach().cpu().permute(1, 2, 0).numpy()
        output_np = output[0].detach().cpu().permute(1, 2, 0).numpy()
        mismatched_ref_np = mismatched_reference[0].detach().cpu().permute(1, 2, 0).numpy()
        # Plot the mismatched visualization:
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))
        axes[0].imshow(noisy_np, cmap='gray')
        axes[0].set_title("Noisy Sketch")
        axes[0].axis("off")

        axes[1].imshow(mismatched_reference[0].cpu().permute(1, 2, 0))
        axes[1].set_title("Mismatched Reference")
        axes[1].axis("off")
        
        axes[2].imshow(warped_np)
        axes[2].set_title("Warped Mismatched Ref")
        axes[2].axis("off")
        
        axes[3].imshow(output_np)
        axes[3].set_title("Output Color")
        axes[3].axis("off")

        plt.tight_layout()
        plt.show()


def visualize_samples_loader(test_loader, ref_unet, denoise_unet, psc_model,
                             device="cuda", num_samples=5):
    """
    Visualize samples from the test DataLoader by generating colorized outputs
    using the provided models.

    Args:
        test_loader (DataLoader): The DataLoader containing test samples.
        ref_unet (nn.Module): The ReferenceUNet model used for extracting features from the reference image.
        denoise_unet (nn.Module): The DenoisingUNet model used for generating the final colorized output.
        psc_model (nn.Module): The PSCNet model used for warping the reference image.
        device (str): The device to run the models on ("cuda" for GPU, "cpu" for CPU).
        num_samples (int): The number of samples to visualize.

    Returns:
        None
    """
    device = torch.device(device if torch.cuda.is_available() else "cpu")

    # Flatten out a few samples from the loader
    # Each batch is (sketch, reference, sketch_transformed, reference_transformed), each shape [B, 3, H, W]
    # zip(*batch) => yields B tuples, each with 4 items of shape [3, H, W]
    test_samples = []
    for batch in test_loader:
        test_samples.extend(zip(*batch))  
        if len(test_samples) >= num_samples:
            break

    dataset_size = len(test_samples)
    if dataset_size == 0:
        print("No samples found in test_loader.")
        return

    num_samples = min(num_samples, dataset_size)
    random_indices = random.sample(range(dataset_size), num_samples)

    for idx in random_indices:
        # sample: (sketch, reference, sketch_transformed, reference_transformed)
        sketch, reference, sketch_transformed, reference_transformed = test_samples[idx]

        # pick a different sample for mismatch
        j = idx
        while j == idx:
            j = random.randint(0, dataset_size - 1)
        mismatched_sketch, mismatched_reference, mismatched_patched_ref, _ = test_samples[j]

        # Add batch dim => shape [1, 3, H, W]
        sketch = sketch.unsqueeze(0).to(device)
        mismatched_reference = mismatched_reference.unsqueeze(0).to(device)
        mismatched_patched_ref = mismatched_patched_ref.unsqueeze(0).to(device)

        # (Optional) add mild noise
        noisy_sketch = torch.clamp(sketch + 0.1*torch.randn_like(sketch), 0., 1.) 
        # shape => [1, 3, H, W]

        # Warp the mismatched reference using PSC
        # shape => [1, 3, H, W]
        warped_ref = warp_reference_with_psc(sketch, mismatched_reference, psc_model)

        # Now we want 9 channels => cat(1) => [1, 9, H, W]
        # Because each is 3 channels
        print("sketch shape:", sketch.shape)                  # Expect [1, 3, H, W]
        print("noisy_sketch shape:", noisy_sketch.shape)      # Expect [1, 3, H, W]
        print("warped_ref shape:", warped_ref.shape)          # Expect [1, 3, H, W]
        print("mismatched_patched_ref shape:", mismatched_patched_ref.shape)  # Expect [1, 3, H, W]
        denoise_input = torch.cat([noisy_sketch, warped_ref, mismatched_patched_ref], dim=1)
        print(f"denoise_input shape: {denoise_input.shape}")  # Should be [1, 9, H, W]

        # Extract reference features
        ref_features = ref_unet(mismatched_patched_ref)

        # Inference
        output = denoise_unet(denoise_input, ref_features)  # shape => [1, 3, H, W]

        # Convert to numpy
        noisy_np = noisy_sketch[0].detach().cpu().permute(1, 2, 0).numpy()
        warped_np = warped_ref[0].detach().cpu().permute(1, 2, 0).numpy()
        output_np = output[0].detach().cpu().permute(1, 2, 0).numpy()
        mismatch_ref_np = mismatched_reference[0].detach().cpu().permute(1, 2, 0).numpy()

        # Plot
        fig, axes = plt.subplots(1, 4, figsize=(16,4))
        axes[0].imshow(noisy_np)
        axes[0].set_title("Noisy Sketch (3ch)")
        axes[0].axis("off")

        axes[1].imshow(mismatch_ref_np)
        axes[1].set_title("Mismatched Ref (3ch)")
        axes[1].axis("off")

        axes[2].imshow(warped_np)
        axes[2].set_title("Warped Ref -> Sketch")
        axes[2].axis("off")

        axes[3].imshow(output_np)
        axes[3].set_title("Output (3ch)")
        axes[3].axis("off")

        plt.tight_layout()
        plt.show()
----
utils/pythonic/__init__.py
def argument_autofilling(func, kwargs):
    """ Arguments auto-filling for function "func"
    Args:
        func    ('function'): function to be filled.
        kwargs  ('dict'):     input
    Returns:
        option  ('dict'):     output
    """
    option = dict()
    kw = func.__code__.co_varnames
    for i in range(func.__code__.co_argcount):
        argname = kw[i]
        if i == 0 and argname == 'self':
            continue
        if argname in kwargs:
            option[argname] = kwargs[argname]
    return option


def get_attributes(_o: object, _name: str, _default=None):
    attributes = _name.split('.')
    o = _o
    for attribute in attributes:
        o = getattr(o, attribute, None)
        if o is None:
            break
    return o

----
utils/logger/__init__.py
from utils.logger.logger import Logger
----
utils/logger/logger.py
import os
import logging
import utils.path


class Logger(object):

    def __init__(
        self,
        name='base',
        level='info',
        console=True,
        logfile='',
        logdir='',
        handler=None,
        format='[%(asctime)s][%(levelname)s] %(message)s',
        datefmt='%m/%d/%Y %H:%M:%S'
    ):
        self.level_mapping = {
            'debug': logging.DEBUG,
            'info': logging.INFO,
            'warning': logging.WARNING,
            'error': logging.ERROR,
            'critical': logging.CRITICAL
        }

        # get basic logger instance
        self._logger = logging.getLogger(name=name)
        self._logger.setLevel(self.level_mapping.get(level))
        logfmt = logging.Formatter(fmt=format, datefmt=datefmt)
        # output to console
        if console:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(logfmt)
            self._logger.addHandler(console_handler)
        # output to log dir
        if logdir and not logfile:
            logfile = os.path.join(logdir, name + '.log')
        # output to logfile
        if logfile:
            utils.path.create_prefix_dir(logfile)
            logfile_handler = logging.FileHandler(filename=logfile, encoding='utf-8')
            logfile_handler.setFormatter(logfmt)
            self._logger.addHandler(logfile_handler)
        # output to handler
        if handler is not None:
            handler.setFormatter(logfmt)
            self._logger.addHandler(handler)

    def debug(self, msg, *args, **kwargs):
        return self._logger.debug(msg, *args, **kwargs)

    def info(self, msg, *args, **kwargs):
        return self._logger.info(msg, *args, **kwargs)

    def warning(self, msg, *args, **kwargs):
        return self._logger.warning(msg, *args, **kwargs)

    def error(self, msg, *args, **kwargs):
        return self._logger.error(msg, *args, **kwargs)

----
utils/module/__init__.py
import torch.nn as nn
import copy


def copy_module(module: nn.Module, requires_grad=None):
    if not module:
        return None
    new_module = copy.deepcopy(module)
    if requires_grad is not None:
        for parameter in new_module.parameters():
            parameter.requires_grad = requires_grad
    return new_module


def zero_module(module: nn.Module):
    if not module:
        return None
    for parameter in module.parameters():
        parameter.detach().zero_()
    return module
----
utils/path/__init__.py
from utils.path.path import *
----
utils/path/path.py
import os
import threading


_util_path_lock = threading.Lock()


def mkdir(dir):
    _util_path_lock.acquire()
    if os.path.exists(dir):
        if not os.path.isdir(dir):
            raise ValueError
    else:
        os.makedirs(dir)
    _util_path_lock.release()


def create_prefix_dir(path: str):
    slash_pos = max(path.rfind('/'), path.rfind('\\'))
    if slash_pos > 0:
        mkdir(path[:slash_pos])

----
utils/image/progressive_patch.py
"""
===========================================================
Progressive Patch Shuffling Utilities
===========================================================

This script contains utility functions for shuffling image patches recursively. It includes the following functions:
1. recursive_patch_shuffle: Shuffle patches of an image recursively and return a PIL image.
2. recursive_patch_shuffle_Tensor: Shuffle patches of an image tensor recursively and return a tensor.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torchvision.transforms as T
import numpy as np

def recursive_patch_shuffle(img, patch_size=16, depth=2):
    """
    Shuffle patches of an image recursively and return a PIL image.

    Args:
        img (PIL.Image): Input image.
        patch_size (int): Size of the patches.
        depth (int): Number of recursive shuffling levels.

    Returns:
        PIL.Image: Image with shuffled patches.
    """
    img_tensor = T.ToTensor()(img)
    _, H, W = img_tensor.shape

    H_pad = (patch_size - H % patch_size) % patch_size
    W_pad = (patch_size - W % patch_size) % patch_size

    # Padding to ensure full patch divisions
    img_tensor = torch.nn.functional.pad(img_tensor, (0, W_pad, 0, H_pad))
    
    def shuffle_patches(tensor, level):
        if level == 0:
            return tensor
        
        C, H, W = tensor.shape
        
        if H < patch_size or W < patch_size:
            return tensor  # Stop if patches are too small

        patches = tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
        patches = patches.permute(1, 2, 0, 3, 4).reshape(-1, C, patch_size, patch_size)
        
        perm = torch.randperm(len(patches))
        shuffled_patches = patches[perm]
        
        shuffled_img = shuffled_patches.view(H // patch_size, W // patch_size, C, patch_size, patch_size)
        shuffled_img = shuffled_img.permute(2, 0, 3, 1, 4).reshape(C, H, W)
        
        return shuffle_patches(shuffled_img, level - 1)
    
    shuffled_tensor = shuffle_patches(img_tensor, depth)
    return T.ToPILImage()(shuffled_tensor)

def recursive_patch_shuffle_Tensor(tenseur, patch_size=16, depth=2):
    """
    Shuffle patches of an image tensor recursively and return a tensor.

    Args:
        tenseur (torch.Tensor): Input image tensor.
        patch_size (int): Size of the patches.
        depth (int): Number of recursive shuffling levels.

    Returns:
        torch.Tensor: Tensor with shuffled patches.
    """
    img_tensor = tenseur
    _, H, W = img_tensor.shape

    H_pad = (patch_size - H % patch_size) % patch_size
    W_pad = (patch_size - W % patch_size) % patch_size

    # Padding to ensure full patch divisions
    img_tensor = torch.nn.functional.pad(img_tensor, (0, W_pad, 0, H_pad))
    
    def shuffle_patches(tensor, level):
        if level == 0:
            return tensor
        
        C, H, W = tensor.shape
        
        if H < patch_size or W < patch_size:
            return tensor  # Stop if patches are too small

        patches = tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
        patches = patches.permute(1, 2, 0, 3, 4).reshape(-1, C, patch_size, patch_size)
        
        perm = torch.randperm(len(patches))
        shuffled_patches = patches[perm]
        
        shuffled_img = shuffled_patches.view(H // patch_size, W // patch_size, C, patch_size, patch_size)
        shuffled_img = shuffled_img.permute(2, 0, 3, 1, 4).reshape(C, H, W)
        
        return shuffle_patches(shuffled_img, level - 1)
    
    shuffled_tensor = shuffle_patches(img_tensor, depth)
    return shuffled_tensor
----
utils/image/xdog.py
import numpy as np
import cv2
from PIL import Image


class XDoG:
    def __init__(self,
                 k_sigma=3.5,
                 p=10,
                 epsilon=0.01,
                 phi=10**9):
        self.sigma_range = [0.3, 0.4, 0.5]
        self.k_sigma = k_sigma
        self.p = p
        self.epsilon = epsilon
        self.phi = phi

    @staticmethod
    def _sharpimage(img: np.array,
                    sigma: float,
                    k_sigma: float,
                    p: float) -> np.array:

        sigma_large = sigma * k_sigma
        G_small = cv2.GaussianBlur(img, (0, 0), sigma)
        G_large = cv2.GaussianBlur(img, (0, 0), sigma_large)
        S = (1+p) * G_small - p * G_large

        return S

    @staticmethod
    def _softthreshold(si: np.array,
                       epsilon: float,
                       phi: float) -> np.array:

        T = np.zeros(si.shape)
        si_bright = si >= epsilon
        si_dark = si < epsilon
        T[si_bright] = 1.0
        T[si_dark] = 1.0 + np.tanh(phi * (si[si_dark] - epsilon))

        return T

    def _xdog(self, img: np.array, sigma: float) -> np.array:
        s = self._sharpimage(img,
                             sigma,
                             self.k_sigma,
                             self.p)
        si = np.multiply(img, s)
        t = self._softthreshold(si, self.epsilon, self.phi)

        return t

    def __call__(self, image):
        img = cv2.cvtColor(np.asarray(image), cv2.COLOR_RGB2GRAY)
        img = img / 255.0
        sigma = np.random.choice(self.sigma_range)
        img = np.rint(self._xdog(img, sigma) * 255.0)
        return Image.fromarray(img)
----
utils/image/compute_flow.py
"""
===========================================================
Optical Flow Computation Utilities
===========================================================

This script contains utility functions for computing and visualizing optical flow between images. It includes the following functions:
1. flow_to_rgb: Convert optical flow to an RGB image.
2. compute_flow: Compute the optical flow between a sketch and a reference image using a given model.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
from torch.nn import functional as F
import numpy as np
import matplotlib.colors as mcolors

def flow_to_rgb(flow):
    """
    Convert optical flow to an RGB image.

    Args:
        flow (torch.Tensor): Optical flow tensor of shape (B, H, W, 2) or (H, W, 2).

    Returns:
        np.ndarray: RGB image representing the optical flow.
    """
    # Move to CPU + NumPy
    flow_np = flow.detach().cpu().numpy()  
    
    # If flow has a batch dimension (B=1), squeeze it out
    if flow_np.ndim == 4:
        # shape is (B, H, W, 2) -> (H, W, 2)
        flow_np = flow_np[0]
    
    fx, fy = flow_np[..., 0], flow_np[..., 1]
    mag = np.sqrt(fx**2 + fy**2)
    ang = np.arctan2(fy, fx) + np.pi
    
    import cv2
    hsv = np.zeros((flow_np.shape[0], flow_np.shape[1], 3), dtype=np.uint8)
    hsv[..., 0] = ang * (180.0 / np.pi / 2.0)  # Hue
    hsv[..., 1] = 255                         # Saturation
    if np.max(mag) < 1e-5:                    # Avoid /0
        hsv[..., 2] = 0
    else:
        hsv[..., 2] = np.clip(mag * 255 / np.max(mag), 0, 255).astype(np.uint8)

    rgb = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)
    return rgb

def compute_flow(model, sketch_of_the_character_tenseur, other_image_of_the_character_tenseur, image_size=256):
    """
    Compute the optical flow between a sketch and a reference image using a given model.

    Args:
        model (torch.nn.Module): Model used to compute the flow.
        sketch_of_the_character_tenseur (torch.Tensor): Sketch image tensor.
        other_image_of_the_character_tenseur (torch.Tensor): Reference image tensor.
        image_size (int): Size of the output image.

    Returns:
        np.ndarray: RGB image representing the optical flow.
    """
    with torch.no_grad():
        photo  = other_image_of_the_character_tenseur.cuda(non_blocking=True)
        sketch = sketch_of_the_character_tenseur.cuda(non_blocking=True)
        if photo.dim() == 3:
            photo = photo.unsqueeze(0)
        if sketch.dim() == 3:
            sketch = sketch.unsqueeze(0)
        _, photo_res = model.encoder_q(photo,   cond=0, return_map=True)
        _, sketch_res= model.encoder_q(sketch, cond=1, return_map=True)
        fwd_flow, bwd_flow = model.forward_stn(photo_res, sketch_res)
        fwd_flow = F.interpolate(
            fwd_flow.permute(0,3,1,2),
            size=(image_size, image_size),
            mode="bilinear", align_corners=True
        ).permute(0,2,3,1)
        photo_warped = F.grid_sample(
            photo, fwd_flow, 
            mode='bilinear', padding_mode='border', align_corners=True
        )
        return flow_to_rgb(fwd_flow)

----
utils/image/warp_psc.py
"""
===========================================================
Warping Utilities Using PSCNet
===========================================================

This script contains utility functions for warping reference images to align with sketches using PSCNet. It includes the following function:
1. warp_reference_with_psc: Warp a reference image to align with a sketch using PSCNet.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torch.nn.functional as F

def warp_reference_with_psc(sketch, reference, psc_model):
    """
    Given a sketch and reference image (both tensors of shape (B, 3, H, W)),
    use PSCNet to compute a forward flow and warp the reference image to align with the sketch.

    Args:
        sketch (torch.Tensor): Sketch image tensor of shape (B, 3, H, W).
        reference (torch.Tensor): Reference image tensor of shape (B, 3, H, W).
        psc_model (torch.nn.Module): PSCNet model used to compute the flow.

    Returns:
        torch.Tensor: Warped reference image tensor of shape (B, 3, H, W).
    """
    with torch.no_grad():
        # PSCNet expects 3-channel inputs and uses a 'cond' flag
        _, ref_map = psc_model.encoder_q(reference, cond=0, return_map=True)
        _, sketch_map = psc_model.encoder_q(sketch, cond=1, return_map=True)
        fwd_flow, _ = psc_model.forward_stn(ref_map, sketch_map)
        # Resize flow to match the sketch/reference dimensions
        fwd_flow = F.interpolate(
            fwd_flow.permute(0, 3, 1, 2),
            size=sketch.shape[-2:],
            mode="bilinear",
            align_corners=True
        ).permute(0, 2, 3, 1)
        warped_reference = F.grid_sample(
            reference, fwd_flow,
            mode='bilinear', padding_mode='border', align_corners=True
        )
    return warped_reference


----
utils/image/CIELab.py
import torch
from torchvision.transforms.functional import to_tensor


class ToCIELab(object):
    def __init__(self, normalize=False) -> None:
        self.f1 = torch.tensor([[ 0.4887180, 0.1762044, 0.0000000], 
                                [ 0.3106803, 0.8129847, 0.0102048], 
                                [ 0.2006017, 0.0108109, 0.9897952]],
                               dtype=torch.float32)
        self.f2 = torch.tensor([[    0,  500,    0],
                                [  116, -500,  200],
                                [    0,    0, -200]],
                               dtype=torch.float32)
        self.normalize = normalize

    def __call__(self, pic):
        """
        Args:
            pic (PIL Image or numpy.ndarray): Image to be converted to tensor in CIELab.

        Returns:
            Tensor: Converted image.
        """
        rgb_tsr = to_tensor(pic=pic).permute(1, 2, 0)
        forward_gamma_seg = rgb_tsr > 0.04045
        rgb_tsr = torch.pow((rgb_tsr + 0.055) / 1.055, 2.4) * forward_gamma_seg + (rgb_tsr / 12.92) * ~forward_gamma_seg
        xyz_tsr = torch.matmul(rgb_tsr, self.f1)
        forward_theta_seg = xyz_tsr > 0.008856
        xyz_tsr = torch.pow(xyz_tsr, 0.3333333) * forward_theta_seg + (xyz_tsr * 7.787 + 0.1379310) * ~forward_theta_seg
        lab_tsr = torch.matmul(xyz_tsr, self.f2)
        lab_tsr[:, :, 0] = torch.clamp(lab_tsr[:, :, 0] - 16.0, min=0., max=100.)
        lab_tsr[:, :, 1:] = torch.clamp(lab_tsr[:, :, 1:], min=-128., max=127.)
        if self.normalize:
            lab_tsr[:, :, 0] = lab_tsr[:, :, 0] / 100.
            lab_tsr[:, :, 1:] = (lab_tsr[:, :, 1:] + 128.) / 255.
        return lab_tsr.permute(2, 0, 1)


class CIELabToRGB(object):
    def __init__(self) -> None:
        self.f2 = torch.tensor([[ 1./116, 1./116, 1./116],
                                [  0.002,      0,      0],
                                [      0,      0, -0.005]],
                               dtype=torch.float32)
        self.f1 = torch.tensor([[ 2.3706743,-0.5138850, 0.0052982],
                                [-0.9000405, 1.4253036,-0.0146949],
                                [-0.4706338, 0.0885814, 1.0093968]],
                               dtype=torch.float32)

    def __call__(self, lab_tsr):
        """
        Args:
            lab_tsr (torch.Tensor): Tensor in CIELab, L \in [0, 100], a, b \in [-128, 127].

        Returns:
            Tensor: Converted tensor [0, 1].
        """
        lab_tsr = torch.clone(lab_tsr).permute(1, 2, 0)
        lab_tsr[:, :, 0] = lab_tsr[:, :, 0] + 16.0
        xyz_tsr = torch.matmul(lab_tsr, self.f2)
        reverse_theta_seg = xyz_tsr > 0.206892672
        xyz_tsr = torch.pow(xyz_tsr, 3) * reverse_theta_seg + ((xyz_tsr - 0.1379310) / 7.787) * ~reverse_theta_seg
        rgb_tsr = torch.matmul(xyz_tsr, self.f1)
        reverse_gamma_seg = rgb_tsr > 0.003130805
        rgb_tsr = (torch.pow(rgb_tsr, 0.416666667) * 1.055 - 0.055) * reverse_gamma_seg + (rgb_tsr * 12.92) * ~reverse_gamma_seg
        return rgb_tsr.permute(2, 0, 1)
----
utils/image/__init__.py
from utils.image.progressive_patch import recursive_patch_shuffle
from utils.image.tensor2PIL import tensor2PIL
from utils.image.CIELab import ToCIELab, CIELabToRGB
from utils.image.xdog import XDoG
from utils.image.plot_image import show_img, plot_img


def is_image(filename: str):
    IMG_EXTENSIONS = ['.jpg', '.JPG', '.jpeg', '.JPEG', '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tif', '.TIF', '.tiff', '.TIFF']
    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)
----
utils/image/load_image.py
"""
===========================================================
Image Loading Utility
===========================================================

This script contains a utility function for loading and transforming images. It includes the following function:
1. load_image: Load an image from a given path and apply transformations.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torchvision.transforms as transforms
from PIL import Image

# === Function: Load Image ===
def load_image(image_path, size=(256, 256)):
    """
    Load an image from a given path and apply transformations.

    Args:
        image_path (str): Path to the image file.
        size (tuple): Desired size of the output image (width, height).

    Returns:
        torch.Tensor: Transformed image tensor of shape (1, 3, H, W).
    """
    transform = transforms.Compose([
        transforms.Resize(size),
        transforms.ToTensor()
    ])
    img = Image.open(image_path).convert("RGB")
    return transform(img).unsqueeze(0)  # (1, 3, H, W)
----
utils/image/tensor2PIL.py
import torch
from torchvision.transforms import ToPILImage


def tensor2PIL(tsr: torch.Tensor):
    assert len(tsr.shape) == 4
    tsr = (torch.clamp(tsr.detach().cpu(), min=-1, max=1).float() + 1.) / 2.
    batch_size = tsr.shape[0]
    ret = []
    for c in range(batch_size):
        ret.append(ToPILImage()(tsr[c]))
    return ret
----
utils/image/shape_transform.py
import os
import random
import math
import numpy as np
from torch.utils.data import Dataset
from PIL import Image
from torchvision.transforms import ToTensor

class ShapeTransform_:
    """
    Class to apply an affine transformation (rotation, scales, shear)
    identically on one or more PIL.Image objects.

    Args:
    - angle_range (tuple): Range for the rotation angle.
    - scale_x_range (tuple): Range for scaling in x.
    - scale_y_range (tuple): Range for scaling in y.
    - shear_x_range (tuple): Range for shearing in x.
    - shear_y_range (tuple): Range for shearing in y.

    Methods:
    - _generate_params(): Generates random transformation parameters.
    - _apply_transform(image, params): Applies the transformation to an image.
    - __call__(*images): Applies the same transformation to the provided images.

    Example:
    >>> transform = ShapeTransform_()
    >>> transformed_image = transform(image)
    """

    def __init__(self, 
                 angle_range=(-180, 180),
                 scale_x_range=(0.8, 1.2),
                 scale_y_range=(0.8, 1.2),
                 shear_x_range=(-20, 20),
                 shear_y_range=(-20, 20)):
        self.angle_range = angle_range
        self.scale_x_range = scale_x_range
        self.scale_y_range = scale_y_range
        self.shear_x_range = shear_x_range
        self.shear_y_range = shear_y_range

    def _generate_params(self):
        """
        Generates random transformation parameters.

        Returns:
        - dict: Dictionary containing the transformation parameters.
        """
        angle = random.uniform(*self.angle_range)
        scale_x = random.uniform(*self.scale_x_range)
        scale_y = random.uniform(*self.scale_y_range)
        shear_x = random.uniform(*self.shear_x_range)
        shear_y = random.uniform(*self.shear_y_range)
        return {
            'angle': angle,
            'scale_x': scale_x,
            'scale_y': scale_y,
            'shear_x': shear_x,
            'shear_y': shear_y
        }

    def _apply_transform(self, image, params):
        """
        Applies the transformation to an image.

        Args:
        - image (PIL.Image): Image to transform.
        - params (dict): Transformation parameters.

        Returns:
        - PIL.Image: Transformed image.
        """
        w, h = image.size
        cx, cy = w / 2, h / 2

        # Convert to radians
        theta = math.radians(params['angle'])
        shear_x_rad = math.radians(params['shear_x'])
        shear_y_rad = math.radians(params['shear_y'])

        # Non-uniform scaling matrix
        S = np.array([
            [params['scale_x'], 0, 0],
            [0, params['scale_y'], 0],
            [0, 0, 1]
        ])

        # Shear matrix
        Sh = np.array([
            [1, math.tan(shear_x_rad), 0],
            [math.tan(shear_y_rad), 1, 0],
            [0, 0, 1]
        ])

        # Rotation matrix
        R = np.array([
            [math.cos(theta), -math.sin(theta), 0],
            [math.sin(theta), math.cos(theta), 0],
            [0, 0, 1]
        ])

        # Combined affine matrix
        M = R @ Sh @ S

        # Translation to apply the transformation around the center
        T1 = np.array([
            [1, 0, -cx],
            [0, 1, -cy],
            [0, 0, 1]
        ])
        T2 = np.array([
            [1, 0, cx],
            [0, 1, cy],
            [0, 0, 1]
        ])
        M_final = T2 @ M @ T1

        # PIL.Image.transform expects the inverse matrix
        M_inv = np.linalg.inv(M_final)
        coeffs = M_inv[:2, :].flatten().tolist()

        # Transformation with bilinear interpolation and white background
        return image.transform((w, h), Image.AFFINE, data=coeffs,
                               resample=Image.BILINEAR, fillcolor=(255, 255, 255))

    def __call__(self, *images):
        """
        Applies the same transformation to the provided images.
        If a single argument is passed, returns the transformed image.
        If multiple images are passed, returns a tuple containing
        each of the transformed images.

        Args:
        - *images: One or more PIL.Image images to transform.

        Returns:
        - PIL.Image or tuple of PIL.Image: Transformed image(s).
        """
        params = self._generate_params()
        results = [self._apply_transform(image, params) for image in images]
        return results[0] if len(results) == 1 else tuple(results)


# Example of usage in a Dataset
class AnimeDataset_(Dataset):
    """
    Dataset loading corresponding images (sketch and reference)
    and applying both a standard transformation (e.g., ToTensor)
    and an identical shape transformation via ShapeTransform.

    The outputs are:
      sketch, reference, sketch_shape_transformed, ref_shape_transformed

    Args:
    - sketch_dir (str): Directory containing the sketch images.
    - reference_dir (str): Directory containing the reference images.
    - transform_standard (callable, optional): Standard transformation to apply (e.g., ToTensor).
    - shape_transform (callable, optional): Instance of ShapeTransform for shape transformations.
    - subset_percentage (float, optional): Percentage of the dataset to use. Default is 1.0.
    - seed (int, optional): Seed for random. Default is 42.

    Methods:
    - __len__(): Returns the size of the dataset.
    - __getitem__(idx): Returns a sample from the dataset.

    Example:
    >>> dataset = AnimeDataset_(sketch_dir="path/to/sketch", reference_dir="path/to/reference")
    >>> sketch, reference, sketch_shape_transformed, ref_shape_transformed = dataset[0]
    """

    def __init__(self, sketch_dir, reference_dir, transform_standard=None, shape_transform=None,
                 subset_percentage=1.0, seed=42):
        self.sketch_dir = sketch_dir
        self.reference_dir = reference_dir
        self.transform_standard = transform_standard  # Standard transformation (e.g., ToTensor)
        self.shape_transform = shape_transform        # Instance of ShapeTransform

        all_images = [f for f in os.listdir(sketch_dir) if f.lower().endswith(('.jpg', '.png'))]
        random.seed(seed)
        random.shuffle(all_images)
        num_images = int(len(all_images) * subset_percentage)
        self.image_names = all_images[:num_images]

    def __len__(self):
        return len(self.image_names)

    def __getitem__(self, idx):
        img_name = self.image_names[idx]
        sketch_path = os.path.join(self.sketch_dir, img_name)
        reference_path = os.path.join(self.reference_dir, img_name)

        # Load images in RGB
        sketch_img = Image.open(sketch_path).convert("RGB")
        reference_img = Image.open(reference_path).convert("RGB")

        # Apply the standard transformation (e.g., ToTensor)
        if self.transform_standard:
            sketch = self.transform_standard(sketch_img)
            reference = self.transform_standard(reference_img)
        else:
            sketch = ToTensor()(sketch_img)
            reference = ToTensor()(reference_img)

        # Apply the identical shape transformation to both images if defined
        if self.shape_transform:
            sketch_shape_transformed, ref_shape_transformed = self.shape_transform(sketch_img, reference_img)
            # Optionally, you can apply the standard transformation to the transformed images
            if self.transform_standard:
                sketch_shape_transformed = self.transform_standard(sketch_shape_transformed)
                ref_shape_transformed = self.transform_standard(ref_shape_transformed)
            else:
                sketch_shape_transformed = ToTensor()(sketch_shape_transformed)
                ref_shape_transformed = ToTensor()(ref_shape_transformed)
        else:
            sketch_shape_transformed = sketch
            ref_shape_transformed = reference

        return sketch, reference, sketch_shape_transformed, ref_shape_transformed


----
utils/image/colored_to_sketch.py
"""
===========================================================
Image Processing Utilities for Sketch Colorization
===========================================================

This script contains utility functions for processing colored images into sketches. It includes the following functions:
1. threshold: Apply a threshold to an image.
2. extract_regions_by_class: Extract pixel regions for each class in a class matrix.
3. enhance_lines_preserving_details: Enhance visible lines in an image while preserving details.
4. get_sketch: Convert a colored image to a sketch.
5. pencil_sketch: Generate a pencil sketch with adjustable parameters.
6. plot_aside: Plot the original image and its sketch side by side.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import numpy as np  # grey images are stored in memory as 2D arrays, color images as 3D arrays
import cv2  # opencv computer vision library
import matplotlib.pyplot as plt  # for plotting images

def threshold(img, value):
    """
    Apply a threshold to an image.

    Args:
        img (np.ndarray): Image to threshold.
        value (int): Threshold value.

    Returns:
        np.ndarray: Thresholded image.
    """
    return np.where(img <= value, 0, 255).astype(np.uint8)

def extract_regions_by_class(classes_matrix, class_list):
    """
    Extract pixel regions for each class in the class matrix.

    Args:
        classes_matrix (np.ndarray): Matrix of class labels.
        class_list (list): List of class numbers to extract.

    Returns:
        dict: Dictionary of masks for each class.
    """
    regions = {}
    for class_num in class_list:
        mask = (classes_matrix == class_num)
        regions[class_num] = mask
    return regions

def enhance_lines_preserving_details(image, alpha=1, beta=1):
    """
    Enhance visible lines in an image while preserving the original details.

    Args:
        image (numpy.ndarray): Input image (BGR format).
        alpha (float): Contrast control (1.0-3.0).
        beta (int): Brightness control (0-100).

    Returns:
        numpy.ndarray: Image with enhanced lines.
    """
    # Convert the image to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply a slight Gaussian blur to reduce noise without oversimplifying details
    blurred = cv2.GaussianBlur(gray, (3, 3), 0)

    # Use the original image and adjust contrast and brightness
    enhanced = cv2.convertScaleAbs(blurred, alpha=alpha, beta=beta)

    # Convert back to 3-channel image for visualization
    result = cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)
    
    return result

def get_sketch(image):
    """
    Convert a colored image to a sketch.

    Args:
        image (numpy.ndarray): Input colored image (BGR format).

    Returns:
        numpy.ndarray: Sketch image.
    """
    # Convert to grayscale
    image_denoised = cv2.fastNlMeansDenoisingColored(image, None, 3, 25, 3, 21)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    inverted = 255 - gray
    blurred = cv2.GaussianBlur(inverted, (5, 5), 0)
    invertedblur = 255 - blurred
    lines = cv2.divide(gray, invertedblur, scale=256.0)
    lines = cv2.cvtColor(lines, cv2.COLOR_GRAY2BGR)
    return threshold(enhance_lines_preserving_details(lines), 254)

def pencil_sketch(image, blur_ksize=31, contrast=1, brightness=5, darkness_percentile=100):
    """
    Generates a pencil sketch with adjustable blur, contrast, brightness, and thresholding.

    Args:
        image (np.ndarray): Input grayscale image.
        blur_ksize (int): Kernel size for Gaussian blur.
        contrast (float): Contrast adjustment factor.
        brightness (int): Brightness adjustment factor.
        darkness_percentile (float): Percentage of darkest pixels to keep (0-100).
        
    Returns:
        np.ndarray: Thresholded pencil sketch.
    """
    # Invert colors
    inverted = cv2.bitwise_not(image)
    
    # Apply Gaussian blur
    blurred = cv2.GaussianBlur(inverted, (blur_ksize, blur_ksize), 0)
    inverted_blurred = cv2.bitwise_not(blurred)
    
    # Create the pencil sketch
    sketch = cv2.divide(image, inverted_blurred, scale=256.0)
    
    # Adjust contrast and brightness
    sketch = cv2.convertScaleAbs(sketch, alpha=contrast, beta=brightness)
    
    # Compute the intensity threshold using percentile
    threshold_value = np.percentile(sketch, darkness_percentile)
    
    # Apply threshold: Keep only pixels darker than the threshold
    sketch[sketch > threshold_value] = 255  # Convert bright pixels to white
    
    return sketch

def plot_aside(img, blur_ksize=31, contrast=1, brightness=5, darkness_percentile=100):
    """
    Plot the original image and its sketch side by side.

    Args:
        img (np.ndarray): Input colored image (BGR format).
        blur_ksize (int): Kernel size for Gaussian blur.
        contrast (float): Contrast adjustment factor.
        brightness (int): Brightness adjustment factor.
        darkness_percentile (float): Percentage of darkest pixels to keep (0-100).
    """
    grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    sketch = pencil_sketch(grey, blur_ksize, contrast, brightness, darkness_percentile)
    # Plot the original image and its sketch
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    axes[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
    axes[0].set_title("Original Image")
    axes[0].axis("off")

    axes[1].imshow(sketch, cmap="gray")
    axes[1].set_title(f"Sketch, blur={blur_ksize}, contrast={contrast}, brightness={brightness}")
    axes[1].axis("off")

    plt.tight_layout()
    plt.show()

----
utils/image/data_augmentation.py
"""
===========================================================
Data Augmentation Utilities for Sketch Colorization
===========================================================

This script contains utility functions for augmenting data by applying random deformations and transformations to images. It includes the following functions:
1. generate_random_deformation_flow: Creates a random smooth deformation field.
2. rotate_and_crop: Applies rotation and zoom-in to avoid black borders.
3. apply_random_transformations: Applies the same deformation flow and random rotation to both reference and sketch images.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import cv2
import os
import numpy as np
import random
import tqdm
from scipy.ndimage import gaussian_filter
from concurrent.futures import ThreadPoolExecutor

# === Function: Generate Random Deformation Flow ===
def generate_random_deformation_flow(h, w, strength=10, noise_level=3, blur_sigma=5):
    """
    Creates a random smooth deformation field.

    Args:
        h (int): Height of the image.
        w (int): Width of the image.
        strength (int): Strength of the deformation.
        noise_level (int): Level of Gaussian noise to add.
        blur_sigma (int): Sigma value for Gaussian blur.

    Returns:
        tuple: Maps for x and y coordinates after deformation.
    """
    x, y = np.meshgrid(np.arange(w), np.arange(h))

    flow_x = strength * np.sin(2 * np.pi * y / h + random.uniform(0, np.pi * 2))
    flow_y = strength * np.cos(2 * np.pi * x / w + random.uniform(0, np.pi * 2))

    # Add random Gaussian noise
    noise_x = np.random.normal(0, noise_level, (h, w))
    noise_y = np.random.normal(0, noise_level, (h, w))

    # Combine smooth flow with noise
    flow_x += noise_x
    flow_y += noise_y

    # Apply Gaussian blur for smoothness
    flow_x = gaussian_filter(flow_x, sigma=blur_sigma)
    flow_y = gaussian_filter(flow_y, sigma=blur_sigma)

    # Clip displacement values
    flow_x = np.clip(flow_x, -strength, strength)
    flow_y = np.clip(flow_y, -strength, strength)

    # Compute new pixel positions
    map_x = np.clip(x + flow_x, 0, w - 1).astype(np.float32)
    map_y = np.clip(y + flow_y, 0, h - 1).astype(np.float32)

    return map_x, map_y

# === Function: Apply Rotation with Zoom ===
def rotate_and_crop(image, angle, zoom=1.1):
    """
    Applies rotation and zoom-in to avoid black borders.

    Args:
        image (np.ndarray): Input image.
        angle (float): Rotation angle in degrees.
        zoom (float): Zoom factor.

    Returns:
        np.ndarray: Rotated and zoomed image.
    """
    h, w = image.shape[:2]
    center = (w // 2, h // 2)

    # Compute transformation matrix with scaling
    M = cv2.getRotationMatrix2D(center, angle, zoom)

    # Apply affine transformation (rotation + zoom)
    rotated = cv2.warpAffine(image, M, (w, h), borderMode=cv2.BORDER_REFLECT)

    return rotated

# === Function: Apply Deformation & Rotation ===
def apply_random_transformations(image, sketch, strength=20, noise_level=10, rotation_range=45, zoom_factor=1.1):
    """
    Applies the same deformation flow and random rotation to both reference and sketch images.

    Args:
        image (np.ndarray): Reference image.
        sketch (np.ndarray): Sketch image.
        strength (int): Strength of the deformation.
        noise_level (int): Level of Gaussian noise to add.
        rotation_range (int): Range of rotation angles.
        zoom_factor (float): Zoom factor.

    Returns:
        tuple: Transformed reference and sketch images.
    """
    h, w, _ = image.shape
    map_x, map_y = generate_random_deformation_flow(h, w, strength, noise_level)
    
    # Apply deformation to both images
    warped_image = cv2.remap(image, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)
    warped_sketch = cv2.remap(sketch, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)

    # Apply random rotation (same for both)
    angle = random.uniform(-rotation_range, rotation_range)
    warped_image = rotate_and_crop(warped_image, angle, zoom_factor)
    warped_sketch = rotate_and_crop(warped_sketch, angle, zoom_factor)

    return warped_image, warped_sketch

----
utils/image/plot_image.py
"""
===========================================================
Image Plotting Utilities
===========================================================

This script contains utility functions for plotting images using Matplotlib. It includes the following functions:
1. show_img: Display a single image on a given axis.
2. plot_img: Plot multiple images in a single figure.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import matplotlib.pyplot as plt
import numpy as np

# === Function: Show Image ===
def show_img(ax, tensor, title=""):
    """
    Display a single image on a given axis.

    Args:
        ax (matplotlib.axes.Axes): Matplotlib axis to display the image on.
        tensor (torch.Tensor): Image tensor to display.
        title (str): Title of the image.
    """
    t = tensor.clamp(0, 1).cpu()
    if t.dim() == 4:
        t = t.squeeze(0)
    if t.shape[0] == 1:
        np_img = t.squeeze(0).numpy()
        ax.imshow(np_img, cmap='gray')
    else:
        np_img = t.permute(1, 2, 0).numpy()
        ax.imshow(np_img)
    ax.set_title(title)
    ax.axis('off')

# === Function: Plot Multiple Images ===
def plot_img(imgs, titles=None, figsize=(12, 12)):
    """
    Plot multiple images in a single figure.

    Args:
        imgs (list of torch.Tensor): List of image tensors to plot.
        titles (list of str, optional): List of titles for each image.
        figsize (tuple): Size of the figure.
    """
    fig, axes = plt.subplots(1, len(imgs), figsize=figsize)
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        title = titles[i] if titles else ""
        show_img(ax, img, title)
    plt.tight_layout()
    plt.show()
----
utils/image/thinplate/__init__.py
from utils.image.thinplate.numpy import *

try:
    import torch
    import utils.image.thinplate.pytorch as torch
except ImportError:
    pass

__version__ = '1.0.0'
----
utils/image/thinplate/numpy.py
# Copyright 2018 Christoph Heindl.
#
# Licensed under MIT License
# ============================================================

import numpy as np

class TPS:       
    @staticmethod
    def fit(c, lambd=0., reduced=False):        
        n = c.shape[0]

        U = TPS.u(TPS.d(c, c))
        K = U + np.eye(n, dtype=np.float32)*lambd

        P = np.ones((n, 3), dtype=np.float32)
        P[:, 1:] = c[:, :2]

        v = np.zeros(n+3, dtype=np.float32)
        v[:n] = c[:, -1]

        A = np.zeros((n+3, n+3), dtype=np.float32)
        A[:n, :n] = K
        A[:n, -3:] = P
        A[-3:, :n] = P.T

        theta = np.linalg.solve(A, v) # p has structure w,a
        return theta[1:] if reduced else theta
        
    @staticmethod
    def d(a, b):
        return np.sqrt(np.square(a[:, None, :2] - b[None, :, :2]).sum(-1))

    @staticmethod
    def u(r):
        return r**2 * np.log(r + 1e-6)

    @staticmethod
    def z(x, c, theta):
        x = np.atleast_2d(x)
        U = TPS.u(TPS.d(x, c))
        w, a = theta[:-3], theta[-3:]
        reduced = theta.shape[0] == c.shape[0] + 2
        if reduced:
            w = np.concatenate((-np.sum(w, keepdims=True), w))
        b = np.dot(U, w)
        return a[0] + a[1]*x[:, 0] + a[2]*x[:, 1] + b

def uniform_grid(shape):
    '''Uniform grid coordinates.
    
    Params
    ------
    shape : tuple
        HxW defining the number of height and width dimension of the grid

    Returns
    -------
    points: HxWx2 tensor
        Grid coordinates over [0,1] normalized image range.
    '''

    H,W = shape[:2]    
    c = np.empty((H, W, 2))
    c[..., 0] = np.linspace(0, 1, W, dtype=np.float32)
    c[..., 1] = np.expand_dims(np.linspace(0, 1, H, dtype=np.float32), -1)

    return c
    
def tps_theta_from_points(c_src, c_dst, reduced=False):
    delta = c_src - c_dst
    
    cx = np.column_stack((c_dst, delta[:, 0]))
    cy = np.column_stack((c_dst, delta[:, 1]))
        
    theta_dx = TPS.fit(cx, reduced=reduced)
    theta_dy = TPS.fit(cy, reduced=reduced)

    return np.stack((theta_dx, theta_dy), -1)


def tps_grid(theta, c_dst, dshape):    
    ugrid = uniform_grid(dshape)

    reduced = c_dst.shape[0] + 2 == theta.shape[0]

    dx = TPS.z(ugrid.reshape((-1, 2)), c_dst, theta[:, 0]).reshape(dshape[:2])
    dy = TPS.z(ugrid.reshape((-1, 2)), c_dst, theta[:, 1]).reshape(dshape[:2])
    dgrid = np.stack((dx, dy), -1)

    grid = dgrid + ugrid
    
    return grid # H'xW'x2 grid[i,j] in range [0..1]

def tps_grid_to_remap(grid, sshape):
    '''Convert a dense grid to OpenCV's remap compatible maps.
    
    Params
    ------
    grid : HxWx2 array
        Normalized flow field coordinates as computed by compute_densegrid.
    sshape : tuple
        Height and width of source image in pixels.


    Returns
    -------
    mapx : HxW array
    mapy : HxW array
    '''

    mx = (grid[:, :, 0] * sshape[1]).astype(np.float32)
    my = (grid[:, :, 1] * sshape[0]).astype(np.float32)

    return mx, my
----
utils/image/thinplate/pytorch.py
# Copyright 2018 Christoph Heindl.
#
# Licensed under MIT License
# ============================================================

import torch

def tps(theta, ctrl, grid):
    '''Evaluate the thin-plate-spline (TPS) surface at xy locations arranged in a grid.
    The TPS surface is a minimum bend interpolation surface defined by a set of control points.
    The function value for a x,y location is given by
    
        TPS(x,y) := theta[-3] + theta[-2]*x + theta[-1]*y + \sum_t=0,T theta[t] U(x,y,ctrl[t])
        
    This method computes the TPS value for multiple batches over multiple grid locations for 2 
    surfaces in one go.
    
    Params
    ------
    theta: Nx(T+3)x2 tensor, or Nx(T+2)x2 tensor
        Batch size N, T+3 or T+2 (reduced form) model parameters for T control points in dx and dy.
    ctrl: NxTx2 tensor or Tx2 tensor
        T control points in normalized image coordinates [0..1]
    grid: NxHxWx3 tensor
        Grid locations to evaluate with homogeneous 1 in first coordinate.
        
    Returns
    -------
    z: NxHxWx2 tensor
        Function values at each grid location in dx and dy.
    '''
    
    N, H, W, _ = grid.size()

    if ctrl.dim() == 2:
        ctrl = ctrl.expand(N, *ctrl.size())
    
    T = ctrl.shape[1]
    
    diff = grid[...,1:].unsqueeze(-2) - ctrl.unsqueeze(1).unsqueeze(1)
    D = torch.sqrt((diff**2).sum(-1))
    U = (D**2) * torch.log(D + 1e-6)

    w, a = theta[:, :-3, :], theta[:, -3:, :]

    reduced = T + 2  == theta.shape[1]
    if reduced:
        w = torch.cat((-w.sum(dim=1, keepdim=True), w), dim=1) 

    # U is NxHxWxT
    b = torch.bmm(U.view(N, -1, T), w).view(N,H,W,2)
    # b is NxHxWx2
    z = torch.bmm(grid.view(N,-1,3), a).view(N,H,W,2) + b
    
    return z

def tps_grid(theta, ctrl, size):
    '''Compute a thin-plate-spline grid from parameters for sampling.
    
    Params
    ------
    theta: Nx(T+3)x2 tensor
        Batch size N, T+3 model parameters for T control points in dx and dy.
    ctrl: NxTx2 tensor, or Tx2 tensor
        T control points in normalized image coordinates [0..1]
    size: tuple
        Output grid size as NxCxHxW. C unused. This defines the output image
        size when sampling.
    
    Returns
    -------
    grid : NxHxWx2 tensor
        Grid suitable for sampling in pytorch containing source image
        locations for each output pixel.
    '''    
    N, _, H, W = size

    grid = theta.new(N, H, W, 3)
    grid[:, :, :, 0] = 1.
    grid[:, :, :, 1] = torch.linspace(0, 1, W)
    grid[:, :, :, 2] = torch.linspace(0, 1, H).unsqueeze(-1)   
    
    z = tps(theta, ctrl, grid)
    return (grid[...,1:] + z)*2-1 # [-1,1] range required by F.sample_grid

def tps_sparse(theta, ctrl, xy):
    if xy.dim() == 2:
        xy = xy.expand(theta.shape[0], *xy.size())

    N, M = xy.shape[:2]
    grid = xy.new(N, M, 3)
    grid[..., 0] = 1.
    grid[..., 1:] = xy

    z = tps(theta, ctrl, grid.view(N,M,1,3))
    return xy + z.view(N, M, 2)

def uniform_grid(shape):
    '''Uniformly places control points aranged in grid accross normalized image coordinates.
    
    Params
    ------
    shape : tuple
        HxW defining the number of control points in height and width dimension

    Returns
    -------
    points: HxWx2 tensor
        Control points over [0,1] normalized image range.
    '''
    H,W = shape[:2]    
    c = torch.zeros(H, W, 2)
    c[..., 0] = torch.linspace(0, 1, W)
    c[..., 1] = torch.linspace(0, 1, H).unsqueeze(-1)
    return c

if __name__ == '__main__':
    c = torch.tensor([
        [0., 0],
        [1., 0],
        [1., 1],
        [0, 1],
    ]).unsqueeze(0)
    theta = torch.zeros(1, 4+3, 2)
    size= (1,1,6,3)
    print(tps_grid(theta, c, size).shape)
----
utils/data/dataset_utils.py
"""
===========================================================
Custom Anime Dataset for Sketch Colorization
===========================================================

This script defines the CustomAnimeDataset class used for loading and processing the anime sketch and reference images for the sketch colorization model. It includes the following steps:
1. Initializing the dataset with paths and parameters
2. Loading and pairing sketch and reference images
3. Applying transformations and generating triplets

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import os
import itertools
from torch.utils.data import Dataset
from PIL import Image
from torchvision.transforms import ToTensor
import random
import torchvision.transforms as T
from utils.image.progressive_patch import recursive_patch_shuffle_Tensor
from utils.image.compute_flow import compute_flow

class CustomAnimeDataset(Dataset):
    def __init__(self, psc, patch_size, sketch_dir, reference_dir, subset_percentage=1.0, seed=42, number_of_ref_per_sketch=5):
        """
        Initialize the CustomAnimeDataset.

        Args:
            psc (object): PSC model for computing flow.
            patch_size (int): Size of the patches for shuffling.
            sketch_dir (str): Directory containing sketch images.
            reference_dir (str): Directory containing reference images.
            subset_percentage (float): Percentage of the dataset to use.
            seed (int): Random seed for reproducibility.
            number_of_ref_per_sketch (int): Number of reference images per sketch.
        """
        self.sketch_dir = sketch_dir
        self.reference_dir = reference_dir
        self.psc = psc
        self.patch_size = patch_size
        self.characters_list = sorted(os.listdir(sketch_dir))
        self.triplets = []
        random.seed(seed)

        for character in self.characters_list:
            sketch_character_path = os.path.join(sketch_dir, character)
            reference_character_path = os.path.join(reference_dir, character)
            if not (os.path.isdir(sketch_character_path) and os.path.isdir(reference_character_path)):
                continue

            # Gather all images for that character
            image_filenames = sorted(os.listdir(sketch_character_path))
            number_of_ref_per_sketch_temp = min(number_of_ref_per_sketch, len(image_filenames) - 1)
            
            if number_of_ref_per_sketch_temp < 1:
                continue # Need at least 2 to form a pair/triplet

            for sketch_filename in image_filenames:
                sketch_img_path = os.path.join(sketch_character_path, sketch_filename)
                reference_img_path = os.path.join(reference_character_path, sketch_filename)
                if not (os.path.exists(sketch_img_path) and os.path.exists(reference_img_path)):
                    continue # Skip if the pair is not complete

                # All possible "others"
                other_references = [f for f in image_filenames if f != sketch_filename]
                if not other_references:
                    continue

                # Randomly take number_of_ref_per_sketch_temp different references
                other_references = random.sample(other_references, number_of_ref_per_sketch_temp) 
                for other_ref_filename in other_references:
                    other_reference_img_path = os.path.join(reference_character_path, other_ref_filename)
                    if os.path.exists(other_reference_img_path):
                        self.triplets.append((sketch_img_path, reference_img_path, other_reference_img_path))

        # Subset if needed
        subset_size = int(len(self.triplets) * subset_percentage)
        self.triplets = random.sample(self.triplets, subset_size)
        print(f"Final number of triplets after applying subset={subset_percentage}: {len(self.triplets)}")

    def __len__(self):
        """Return the total number of triplets."""
        return len(self.triplets)

    def __getitem__(self, idx):
        """
        Get a triplet of images (sketch, reference, other reference) and their transformations.

        Args:
            idx (int): Index of the triplet.

        Returns:
            tuple: Transformed sketch, reference, other reference, patched reference, and flow tensors.
        """
        sketch_path, reference_path, other_reference_path = self.triplets[idx]

        # Load images in RGB
        sketch_img = Image.open(sketch_path).convert("RGB")
        reference_img = Image.open(reference_path).convert("RGB")
        other_reference_img = Image.open(other_reference_path).convert("RGB")

        transform = T.Compose([
            T.Resize((256, 256)), 
            T.ToTensor()
        ])
        sketch_tensor = transform(sketch_img)
        reference_tensor = transform(reference_img)
        other_reference_tensor = transform(other_reference_img)
        patched_ref_tensor = recursive_patch_shuffle_Tensor(other_reference_tensor, patch_size=self.patch_size, depth=2)
        flow_rgb = compute_flow(self.psc, sketch_tensor, other_reference_tensor, image_size=256)
        flow_pil = Image.fromarray(flow_rgb)  
        flow_tensor = transform(flow_pil)     

        return (sketch_tensor, reference_tensor, other_reference_tensor, patched_ref_tensor, flow_tensor)
----
models/attention.py
"""
===========================================================
Attention Mechanisms for Neural Networks
===========================================================

This script contains various attention mechanisms used in neural networks. It includes the following classes:
1. ChannelAttention: Implements the Channel Attention mechanism.
2. SpatialAttention: Implements the Spatial Attention mechanism.
3. CBAM: Combines both Channel and Spatial Attention mechanisms.
4. SimpleSelfAttention: Implements a simple self-attention mechanism.
5. RefUNetAttentionBlock: Implements a comprehensive attention block similar to modern transformer blocks.
6. CrossAttentionBlock: Implements a cross-attention block for integrating features from another source.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F

class ChannelAttention(nn.Module):
    """
    This class implements the Channel Attention mechanism.
    It focuses on the importance of each channel in the feature map.

    Args:
    - in_channel (int): Number of input channels.
    - ratio (int): Reduction ratio for the intermediate layer (default is 16).

    Methods:
    - forward(x): Forward pass of the channel attention mechanism.

    Example:
    >>> ca = ChannelAttention(in_channel=64)
    >>> output = ca(input_tensor)
    """

    def __init__(self, in_channel, ratio=16):
        super(ChannelAttention, self).__init__()
        self.max_pool = nn.AdaptiveMaxPool2d(1)  # Global max pooling
        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # Global average pooling
        self.fc = nn.Sequential(
            nn.Linear(in_channel, in_channel // ratio, False),  # Fully connected layer with reduction ratio
            nn.ReLU(),  # ReLU activation
            nn.Linear(in_channel // ratio, in_channel, False)  # Fully connected layer to restore original channel size
        )
        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to get attention weights

    def forward(self, x):
        b, c, h, w = x.size()
        max_pool_out = self.max_pool(x).view(b, c)  # Apply max pooling and reshape
        avg_pool_out = self.avg_pool(x).view(b, c)  # Apply average pooling and reshape

        max_fc_out = self.fc(max_pool_out)  # Pass max pooled output through fully connected layers
        avg_fc_out = self.fc(avg_pool_out)  # Pass average pooled output through fully connected layers

        out = max_fc_out + avg_fc_out  # Combine the outputs
        out = self.sigmoid(out).view(b, c, 1, 1)  # Apply sigmoid and reshape to match input dimensions

        return x * out  # Multiply input by attention weights


class SpatialAttention(nn.Module):
    """
    This class implements the Spatial Attention mechanism.
    It focuses on the importance of each spatial location in the feature map.

    Args:
    - kernel_size (int): Size of the convolutional kernel (default is 7).

    Methods:
    - forward(x): Forward pass of the spatial attention mechanism.

    Example:
    >>> sa = SpatialAttention(kernel_size=7)
    >>> output = sa(input_tensor)
    """

    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, stride=1, padding=kernel_size//2, bias=False)  # Convolutional layer
        self.sigmoid = nn.Sigmoid()  # Sigmoid activation to get attention weights

    def forward(self, x):
        max_pool_out, _ = torch.max(x, dim=1, keepdim=True)  # Max pooling along the channel dimension
        mean_pool_out = torch.mean(x, dim=1, keepdim=True)  # Mean pooling along the channel dimension
        pool_out = torch.cat([max_pool_out, mean_pool_out], dim=1)  # Concatenate max and mean pooled outputs

        out = self.conv(pool_out)  # Apply convolution
        out = self.sigmoid(out)  # Apply sigmoid to get attention weights

        return x * out  # Multiply input by attention weights


class CBAM(nn.Module):
    """
    This class implements the Convolutional Block Attention Module (CBAM).
    It combines both Channel Attention and Spatial Attention mechanisms.

    Args:
    - in_channel (int): Number of input channels.
    - ratio (int): Reduction ratio for the Channel Attention (default is 16).
    - kernel_size (int): Size of the convolutional kernel for the Spatial Attention (default is 7).

    Methods:
    - forward(x): Forward pass of the CBAM.

    Example:
    >>> cbam = CBAM(in_channel=64)
    >>> output = cbam(input_tensor)
    """

    def __init__(self, in_channel, ratio=16, kernel_size=7):
        super(CBAM, self).__init__()
        self.channel_attention = ChannelAttention(in_channel, ratio)  # Initialize Channel Attention
        self.spatial_attention = SpatialAttention(kernel_size)  # Initialize Spatial Attention

    def forward(self, x):
        x = self.channel_attention(x)  # Apply Channel Attention
        x = self.spatial_attention(x)  # Apply Spatial Attention

        return x  # Return the output with attention applied


class SimpleSelfAttention(nn.Module):
    """
    This class implements a simple self-attention mechanism.
    It focuses on capturing long-range dependencies in the feature map.

    Args:
    - in_channels (int): Number of input channels.

    Methods:
    - forward(x): Forward pass of the self-attention mechanism.

    Example:
    >>> sa = SimpleSelfAttention(in_channels=64)
    >>> output = sa(input_tensor)
    """

    def __init__(self, in_channels):
        super().__init__()
        self.query = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.key   = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.value = nn.Conv2d(in_channels, in_channels, kernel_size=1)
        self.gamma = nn.Parameter(torch.zeros(1))
        
    def forward(self, x):
        B, C, H, W = x.size()
        proj_query = self.query(x).view(B, -1, H * W).permute(0, 2, 1)  # (B, N, C//8)
        proj_key   = self.key(x).view(B, -1, H * W)                       # (B, C//8, N)
        energy     = torch.bmm(proj_query, proj_key)                      # (B, N, N)
        attention  = F.softmax(energy, dim=-1)
        proj_value = self.value(x).view(B, -1, H * W)                       # (B, C, N)
        out = torch.bmm(proj_value, attention.permute(0, 2, 1))             # (B, C, N)
        out = out.view(B, C, H, W)
        out = self.gamma * out + x
        return out


class RefUNetAttentionBlock(nn.Module):
    """
    This class implements a more comprehensive attention block that integrates not only an attention layer,
    but also normalization, residual connections, and a feed-forward MLP, making it more similar to modern transformer blocks.
    It helps capture global dependencies and refine the representation compared to simple self-attention.

    Args:
    - dim (int): Dimension of the input (and output) channels.
    - num_heads (int): Number of heads for multi-head attention.
    - dropout (float, optional): Dropout rate to apply in attention and MLP. Default is 0.0.

    Methods:
    - forward(x): Forward pass of the attention block.

    Example:
    >>> attn_block = RefUNetAttentionBlock(dim=64, num_heads=8)
    >>> output = attn_block(input_tensor)
    """

    def __init__(self, dim, num_heads, dropout=0.0):
        super().__init__()
        self.dim = dim
        self.num_heads = num_heads

        # Normalization before self-attention
        self.norm1 = nn.LayerNorm(dim)
        # Multi-head attention module from PyTorch (expects a sequence)
        self.self_attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout)
        
        # Normalization before the MLP
        self.norm2 = nn.LayerNorm(dim)
        # Transformer-like feed-forward network (MLP)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout)
        )
        
    def forward(self, x):
        """
        Forward pass of the attention block.

        Args:
        - x (torch.Tensor): Input tensor of shape (B, C, H, W).

        Returns:
        - torch.Tensor: Output tensor after applying the attention block.
        """
        B, C, H, W = x.size()
        # Flatten the spatial dimension to get a sequence: (seq_len, batch, dim)
        x_seq = x.view(B, C, H * W).permute(2, 0, 1)
        
        # Self-attention
        x_norm = self.norm1(x_seq)
        attn_out, _ = self.self_attn(x_norm, x_norm, x_norm)
        x_res = x_seq + attn_out  # Residual connection
        
        # MLP (feed-forward)
        x_norm2 = self.norm2(x_res)
        mlp_out = self.mlp(x_norm2)
        x_res2 = x_res + mlp_out  # Another residual connection
        
        # Reshape back to (batch, dim, H, W)
        out = x_res2.permute(1, 2, 0).view(B, C, H, W)
        return out


class CrossAttentionBlock(nn.Module):
    """
    This class implements a cross-attention block that allows the model to attend to features from another source.
    It integrates features from a reference image into the main image features.

    Args:
    - dim (int): Dimension of the input (and output) channels.
    - cross_dim (int, optional): Dimension of the cross-attention channels. Default is None.
    - num_heads (int): Number of heads for multi-head attention.
    - dropout (float, optional): Dropout rate to apply in attention and MLP. Default is 0.0.

    Methods:
    - forward(x, cross): Forward pass of the cross-attention block.

    Example:
    >>> cross_attn_block = CrossAttentionBlock(dim=64, cross_dim=128, num_heads=4)
    >>> output = cross_attn_block(input_tensor, cross_tensor)
    """

    def __init__(self, dim, cross_dim=None, num_heads=4, dropout=0.0):
        super().__init__()
        self.dim = dim
        self.cross_dim = cross_dim if cross_dim is not None else dim
        self.num_heads = num_heads
        self.norm_query = nn.LayerNorm(dim)
        # Projection to adapt reference features if necessary
        if self.cross_dim != dim:
            self.cross_proj = nn.Linear(self.cross_dim, dim)
        else:
            self.cross_proj = nn.Identity()
        self.norm_cross = nn.LayerNorm(dim)
        self.attn = nn.MultiheadAttention(embed_dim=dim, num_heads=num_heads, dropout=dropout, batch_first=True)
        self.norm_out = nn.LayerNorm(dim)
        self.mlp = nn.Sequential(
            nn.Linear(dim, dim * 4),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(dim * 4, dim),
            nn.Dropout(dropout)
        )
    
    def forward(self, x, cross):
        """
        Forward pass of the cross-attention block.

        Args:
        - x (torch.Tensor): Input tensor of shape (B, C, H, W).
        - cross (torch.Tensor): Cross-attention tensor of shape (B, cross_channels, H, W).

        Returns:
        - torch.Tensor: Output tensor after applying the cross-attention block.
        """
        B, C, H, W = x.shape
        # Ensure cross has the same spatial resolution as x
        if cross.shape[-2:] != (H, W):
            cross = F.interpolate(cross, size=(H, W), mode="nearest").contiguous()
        # cross is assumed to be of shape (B, cross_channels, H, W)
        # Transform it into a sequence (B, H*W, cross_channels)
        cross_seq = cross.view(B, cross.shape[1], H * W).permute(0, 2, 1)
        # Apply the projection (defined in __init__) to get the expected dimension
        cross_seq = self.cross_proj(cross_seq)
        # Transform x into a sequence (B, H*W, C)
        x_seq = x.view(B, C, H * W).permute(0, 2, 1)
        x_norm = self.norm_query(x_seq)
        cross_norm = self.norm_cross(cross_seq)
        attn_out, _ = self.attn(query=x_norm, key=cross_norm, value=cross_norm)
        x_res = x_seq + attn_out
        x_norm2 = self.norm_out(x_res)
        mlp_out = self.mlp(x_norm2)
        x_final = x_res + mlp_out
        # Reshape back to (B, C, H, W)
        return x_final.permute(0, 2, 1).view(B, C, H, W)
----
models/reference_unet.py
"""
===========================================================
Reference UNet for Sketch Colorization
===========================================================

This script contains the implementation of the Reference UNet model used for encoding reference images in the context of anime diffusion models. It includes the following class:
1. ReferenceUNet: Implements a U-Net-like architecture with residual blocks and attention mechanisms.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.residual_block import ResidualBlock
from models.attention import RefUNetAttentionBlock
from utils.image import progressive_patch
from torchvision import transforms as T

class ReferenceUNet(nn.Module):
    """
    This class implements a U-Net-like architecture with residual blocks and attention mechanisms.
    It is used for encoding reference images in the context of anime diffusion models.

    Args:
    - in_channels (int): Number of input channels (typically 3 for RGB).
    - base_ch (int): Number of base channels.
    - num_res_blocks (int): Number of residual blocks per level.
    - num_attn_blocks (int): Number of attention blocks in the bottleneck.
    - cbam (bool): Whether to use CBAM (Convolutional Block Attention Module). Default is False.
    - device (str): Device to run the model on. Default is 'cuda'.

    Methods:
    - set_to_shuffle(to_shuffle): Set whether to apply patch shuffle.
    - forward(ref_img, patch_size): Forward pass of the network.

    Example:
    >>> model = ReferenceUNet(in_channels=3, base_ch=64, num_res_blocks=3, num_attn_blocks=3)
    >>> output = model(input_tensor)
    """

    def __init__(self, in_channels=3, base_ch=64, num_res_blocks=3, num_attn_blocks=3, cbam=False, device=torch.device('cuda')):
        super().__init__()

        # Level 1: Initial encoding with several residual blocks
        self.enc1 = nn.Sequential(
            nn.Conv2d(in_channels, base_ch, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(base_ch),
            nn.ReLU(inplace=True),
            *[ResidualBlock(base_ch, base_ch, cbam=cbam) for _ in range(num_res_blocks)]
        )
        
        # Level 2: Downsampling and encoding
        self.down1 = nn.Sequential(
            nn.Conv2d(base_ch, base_ch * 2, kernel_size=4, stride=2, padding=1),  # Downsample by 2
            nn.BatchNorm2d(base_ch * 2),
            nn.ReLU(inplace=True),
            *[ResidualBlock(base_ch * 2, base_ch * 2, cbam=cbam) for _ in range(num_res_blocks)]
        )
        
        # Level 3: Downsampling and encoding
        self.down2 = nn.Sequential(
            nn.Conv2d(base_ch * 2, base_ch * 4, kernel_size=4, stride=2, padding=1),  # Downsample by 2
            nn.BatchNorm2d(base_ch * 4),
            nn.ReLU(inplace=True),
            *[ResidualBlock(base_ch * 4, base_ch * 4, cbam=cbam) for _ in range(num_res_blocks)]
        )
        
        # Skip connection: project x2 to add to x3
        self.skip_conv = nn.Conv2d(base_ch * 2, base_ch * 4, kernel_size=1) # TO REMOVE ? 
        
        # Attention block(s) in the bottleneck
        self.attn_blocks = nn.Sequential(
            *[RefUNetAttentionBlock(base_ch * 4, num_heads=4, dropout=0.1) for _ in range(num_attn_blocks)]
        )
        
        # Final refinement layer
        self.final_conv = nn.Sequential(
            nn.Conv2d(base_ch * 4, base_ch * 4, kernel_size=3, padding=1),
            nn.BatchNorm2d(base_ch * 4),
            nn.ReLU(inplace=True)
        )

        self.device = device

    def forward(self, ref_patch):
        """
        Forward pass of the network.

        Args:
        - ref_img (PIL Image or Tensor): Reference image.
        - patch_size (int): Size of the patches for patch shuffle.

        Returns:
        - torch.Tensor: Encoded features.
        """
        x = ref_patch.to(self.device)
    
        # Encoding levels
        x1 = self.enc1(x)
        x2 = self.down1(x1)
        x3 = self.down2(x2)

        # Skip connection
        skip = self.skip_conv(x2)
        skip = F.avg_pool2d(skip, kernel_size=2)
        x3 = x3 + skip

        # Attention blocks
        x3 = self.attn_blocks(x3)

        # Final refinement
        feats = self.final_conv(x3)
        return feats

----
models/__init__.py
from models.simple_dualunet_dataset import DualUNetDataset
from models.simple_cross_attention import SimpleCrossAttention  # Ensure this module exists
from models.simple_denoising_unet import DenoisingUNet
from models.simple_refunet import ReferenceUNet

----
models/residual_block.py
"""
===========================================================
Residual Block for Sketch Colorization
===========================================================

This script contains the implementation of the Residual Block used in the context of anime diffusion models. It includes the following class:
1. ResidualBlock: Implements a residual block with optional attention mechanisms.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch.nn as nn
from models.attention import CBAM

class ResidualBlock(nn.Module):
    """
    This class implements a residual block with optional attention mechanisms.
    It is used in the context of anime diffusion models.

    Args:
    - channel_in (int): Number of input channels.
    - channel_out (int): Number of output channels.
    - time_channel (int, optional): Dimension of the time embeddings. Default is None.
    - dropout (float, optional): Dropout rate. Default is 0.
    - cbam (bool, optional): Whether to use CBAM (Convolutional Block Attention Module). Default is False.

    The residual block consists of:
    - Two convolutional layers with GroupNorm and SiLU activations.
    - An optional time embedding layer.
    - An optional CBAM attention mechanism.
    - A skip connection to add the input to the output.
    """

    def __init__(
        self,
        channel_in,
        channel_out,
        time_channel=None,
        dropout=0,
        cbam=False
    ):
        super().__init__()

        # Member variables
        self.channel_base = channel_in
        self.channel_out = channel_out

        # Time embedding layer (optional)
        self.time_emb = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_channel, self.channel_out)
        ) if time_channel is not None else None

        # First convolutional layer with GroupNorm and SiLU activation
        self.conv1 = nn.Sequential(
            nn.GroupNorm(32, self.channel_base),
            nn.SiLU(),
            nn.Conv2d(self.channel_base, self.channel_out, kernel_size=3, padding=1)
        )

        # Second convolutional layer with GroupNorm, SiLU activation, and Dropout
        self.conv2 = nn.Sequential(
            nn.GroupNorm(32, self.channel_out),
            nn.SiLU(),
            nn.Dropout(p=dropout),
            nn.Conv2d(self.channel_out, self.channel_out, kernel_size=3, padding=1)
        )

        # Attention mechanism (optional)
        self.attention = nn.Sequential(
            CBAM(self.channel_out)
        ) if cbam else nn.Identity()

        # Skip connection
        if self.channel_base == self.channel_out:
            self.skip_connection = nn.Identity()
        else:
            self.skip_connection = nn.Conv2d(self.channel_base, self.channel_out, kernel_size=1)

    def forward(self, x, t=None):
        """
        Forward pass of the residual block.

        Args:
        - x (torch.Tensor): Input tensor of shape [batch_size, x_channels, height, width].
        - t (torch.Tensor, optional): Time embedding tensor of shape [batch_size, t_embedding]. Default is None.

        Returns:
        - torch.Tensor: Output tensor after applying the residual block.
        """
        # Apply the first convolutional layer
        h = self.conv1(x)

        # Add time embedding if provided
        if t is not None and self.time_emb is not None:
            h += self.time_emb(t)[:, :, None, None]

        # Apply the attention mechanism
        h = self.attention(h)

        # Apply the second convolutional layer
        h = self.conv2(h)

        # Add the skip connection
        return self.skip_connection(x) + h
----
models/psc_diffusion.py
"""
===========================================================
PSC Gaussian Diffusion Model for Sketch Colorization
===========================================================

This script contains the implementation of the PSC Gaussian Diffusion model used for sketch colorization. It includes the following components:
1. L2_loss: Compute the L2 loss (mean squared error) between the input and target tensors.
2. extract: Extract values from a tensor at specified indices and reshape to match a given shape.
3. PSCGaussianDiffusion: Implements the Gaussian diffusion model with PSC (Patch-based Spatial Consistency) for image denoising.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from functools import partial
import numpy as np
from utils.image.warp_psc import warp_reference_with_psc

def L2_loss(input, target):
    """
    Compute the L2 loss (mean squared error) between the input and target tensors.

    Args:
        input (torch.Tensor): The input tensor.
        target (torch.Tensor): The target tensor.

    Returns:
        torch.Tensor: The computed L2 loss.
    """
    return F.mse_loss(input, target, reduction="mean")

def extract(a, t, x_shape):
    """
    Extract values from a tensor `a` at indices specified by `t` and reshape to match `x_shape`.

    Args:
        a (torch.Tensor): The tensor to extract values from.
        t (torch.Tensor): The indices to extract.
        x_shape (tuple): The shape to reshape the extracted values to.

    Returns:
        torch.Tensor: The extracted and reshaped values.
    """
    batch_size = t.shape[0]
    out = a.to(t.device).gather(0, t).float()
    out = out.reshape(batch_size, *((1,) * (len(x_shape) - 1)))
    return out

class PSCGaussianDiffusion(nn.Module):
    """
    This class implements a Gaussian diffusion model with PSC (Patch-based Spatial Consistency) for image denoising.
    It uses a denoising UNet, a reference UNet, and a PSC model for feature extraction and transformation.

    Args:
        time_step (int): Number of time steps for the diffusion process.
        betas (dict): Dictionary containing the start and end values for the beta schedule.
        denoising_unet (nn.Module): The denoising UNet model.
        psc_model (nn.Module): The PSC model for patch-based spatial consistency.

    Methods:
        q_sample(x_0, t, noise): Sample from the diffusion process.
        p_sample(x_t, t, ref_feats, x_cond, eta): Sample from the reverse diffusion process.
        inference(x_t, ref_feats, x_cond, eta): Perform inference using the reverse diffusion process.
        forward(image_of_the_character, t, sketch_of_the_character, flow_warped, ref_feats): Forward pass of the diffusion model.
    """

    def __init__(self, time_step, betas, denoising_unet, psc_model):
        super().__init__()
        self.denoise_fn = denoising_unet
        self.psc_model = psc_model
        self.time_steps = time_step

        # Beta schedule
        scale = 1000 / self.time_steps
        betas = torch.linspace(
            scale * betas['linear_start'],
            scale * betas['linear_end'],
            self.time_steps,
            dtype=torch.float32
        )
        alphas = 1. - betas
        gammas = torch.cumprod(alphas, axis=0)
        gammas_prev = F.pad(gammas[:-1], (1,0), value=1.)

        # Register buffers
        self.register_buffer('gammas', gammas)
        self.register_buffer('sqrt_reciprocal_gammas', torch.sqrt(1. / gammas))
        self.register_buffer('sqrt_reciprocal_gammas_m1', torch.sqrt(1. / gammas - 1))

        posterior_variance = betas * (1. - gammas_prev) / (1. - gammas)
        self.register_buffer('posterior_variance', posterior_variance)
        self.register_buffer('posterior_log_variance', torch.log(torch.clamp(posterior_variance, min=1e-20)))
        self.register_buffer('posterior_mean_coef1', betas * torch.sqrt(gammas_prev) / (1. - gammas))
        self.register_buffer('posterior_mean_coef2', (1. - gammas_prev) * torch.sqrt(alphas) / (1. - gammas))

        self.loss_fn = partial(F.mse_loss, reduction="sum")

    def q_sample(self, x_0, t, noise=None):
        """
        Sample from the diffusion process. q(x_t | x_0)

        Args:
            x_0 (torch.Tensor): The original image tensor.
            t (torch.Tensor): The time step tensor.
            noise (torch.Tensor, optional): The noise tensor. Default is None.

        Returns:
            torch.Tensor: The noisy image tensor.
        """
        if noise is None:
            noise = torch.randn_like(x_0)
        gammas_t = extract(self.gammas, t, x_0.shape).to(x_0.device)
        return torch.sqrt(gammas_t) * x_0 + torch.sqrt(1 - gammas_t) * noise

    @torch.no_grad()
    def p_sample(self, x_t, t, ref_feats, x_cond=None, eta=1):
        """
        Sample from the reverse diffusion process.

        Args:
            x_t (torch.Tensor): The noisy image tensor.
            t (torch.Tensor): The time step tensor.
            ref_feats (torch.Tensor): The reference features tensor.
            x_cond (torch.Tensor, optional): The conditioning tensor. Default is None.
            eta (float, optional): The noise scale factor. Default is 1.

        Returns:
            torch.Tensor: The denoised image tensor.
        """
        if x_cond is not None:
            # Concatenate x_t and x_cond along the channel dimension
            x_in = torch.cat([x_t, x_cond], dim=1)
            predicted_noise = self.denoise_fn(x_in, t, ref_feats=ref_feats)
        else:
            predicted_noise = self.denoise_fn(x_t, t, ref_feats=ref_feats)
        predicted_x_0 = extract(self.sqrt_reciprocal_gammas, t, x_t.shape) * x_t - extract(self.sqrt_reciprocal_gammas_m1, t, x_t.shape) * predicted_noise
        predicted_x_0 = torch.clamp(predicted_x_0, min=-1., max=1.)
        posterior_mean = extract(self.posterior_mean_coef1, t, x_t.shape) * predicted_x_0 + extract(self.posterior_mean_coef2, t, x_t.shape) * x_t
        posterior_log_variance = extract(self.posterior_log_variance, t, x_t.shape)
        noise = torch.randn_like(x_t)
        nonzero_mask = eta * ((t != 0).float().view(-1, *([1] * (len(x_t.shape) - 1))))
        return posterior_mean + nonzero_mask * (0.5 * posterior_log_variance).exp() * noise
    
    @torch.no_grad()
    def inference(self, x_t, ref_feats, x_cond=None, eta=1):
        """
        Perform inference using the reverse diffusion process.

        Args:
            x_t (torch.Tensor): The noisy image tensor.
            ref_feats (torch.Tensor): The reference features tensor.
            x_cond (torch.Tensor, optional): The conditioning tensor. Default is None.
            eta (float, optional): The noise scale factor. Default is 1.

        Returns:
            list of torch.Tensor: The list of denoised image tensors at each time step.
        """
        batch_size = 1
        device = next(self.parameters()).device
        ret = []
        for i in reversed(range(0, self.time_steps)):
            x_t = self.p_sample(x_t=x_t, t=torch.full((batch_size, ), i, device=device, dtype=torch.long), ref_feats=ref_feats, x_cond=x_cond, eta=eta)
            ret.append(x_t.cpu())
        return ret

    def forward(self, image_of_the_character, t, sketch_of_the_character, flow_warped, ref_feats):
        """
        Forward pass of the diffusion model.

        Args:
            image_of_the_character (torch.Tensor): The original reference image tensor.
            t (torch.Tensor): The time step tensor.
            sketch_of_the_character (torch.Tensor): The sketch image tensor.
            flow_warped (torch.Tensor): The warped reference image tensor.
            ref_feats (torch.Tensor): The reference features tensor.

        Returns:
            torch.Tensor: The computed diffusion loss.
        """
        # noise
        noise = torch.randn_like(image_of_the_character)

        # q_sample => x_noisy
        x_noisy = self.q_sample(image_of_the_character, t, noise)

        # build a 6-channel condition from (sketch, warped_ref)
        x_cond = torch.cat([sketch_of_the_character, flow_warped], dim=1) # Conditioning on the ref and the deformation flow

        # cat x_noisy [B,3,H,W] with x_cond [B,6,H,W] => (B,9,H,W)
        x_in = torch.cat([x_noisy, x_cond], dim=1)

        # pass to denoising net
        noise_tilde = self.denoise_fn(x_in, t, ref_feats=ref_feats)

        # standard diffusion loss on noise
        diff_loss = self.loss_fn(noise, noise_tilde)

        return diff_loss

----
models/denoising_unet.py
"""
===========================================================
Denoising UNet for Sketch Colorization
===========================================================

This script contains the implementation of the Denoising UNet model used for sketch colorization. It includes the following classes:
1. SampleBlock: Implements a sampling block for upsampling or downsampling.
2. DenoisingUNet: Implements the Denoising UNet with residual blocks and cross-attention mechanisms.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from models.components.xt_module import Module, Sequential
from models.residual_block import ResidualBlock
from models.attention import CrossAttentionBlock


class SampleBlock(Module):
    """
    This class implements a sampling block for upsampling or downsampling.
    It is used in the context of the Denoising UNet.

    Args:
    - sampling_type (str, optional): Type of sampling ('up' for upsampling, 'down' for downsampling). Default is None.

    Methods:
    - forward(x, t): Forward pass of the sampling block.

    Example:
    >>> sample_block = SampleBlock(sampling_type="up")
    >>> output = sample_block(input_tensor)
    """

    def __init__(self, sampling_type=None):
        super().__init__()

        # Member variables
        if sampling_type == "up":
            self.sampling = nn.Upsample(scale_factor=2, mode="nearest")
        elif sampling_type == "down":
            self.sampling = nn.AvgPool2d(kernel_size=2, stride=2)
        else:
            self.sampling = nn.Identity()

    def forward(self, x, t=None):
        return self.sampling(x)


class DenoisingUNet(nn.Module):
    """
    This class implements a Denoising UNet with residual blocks and cross-attention mechanisms.
    It is used for denoising images in the context of anime diffusion models.

    Args:
    - channel_in (int): Number of input channels.
    - channel_out (int, optional): Number of output channels. Default is None.
    - channel_base (int): Number of base channels.
    - channel_features (int): Dimension of the reference features for cross-attention.
    - n_res_blocks (int): Number of residual blocks per level.
    - dropout (float): Dropout rate.
    - channel_mult (tuple): Multipliers for the number of channels at each level.
    - attention_head (int): Number of attention heads.
    - cbam (bool): Whether to use CBAM (Convolutional Block Attention Module). Default is True.

    Methods:
    - forward(x, t, ref_feats): Forward pass of the network.

    Example:
    >>> model = DenoisingUNet(channel_in=3, channel_out=3, channel_base=64, channel_features=64)
    >>> output = model(input_tensor, t_tensor, ref_feats_tensor)
    """

    def __init__(self, channel_in, channel_out=None, channel_base=64, channel_features=64,
                 n_res_blocks=2, dropout=0, channel_mult=(1, 2, 4, 8), attention_head=4, cbam=True):
        super().__init__()
        self.channel_in = channel_in
        self.channel_out = channel_out or channel_in
        self.channel_base = channel_base
        self.channel_features = channel_features  # Expected dimension of ref_feats in the cross-attention

        # Time embedding layer
        time_embedding_channel = channel_base * 4
        self.time_embedding = nn.Sequential(
            nn.Linear(self.channel_base, time_embedding_channel),
            nn.SiLU(),
            nn.Linear(time_embedding_channel, time_embedding_channel)
        )

        # Input layer
        self.input = nn.Sequential(
            nn.Conv2d(self.channel_in, self.channel_base, kernel_size=1)
        )

        # Encoder blocks
        channel_sequence = [channel_base]
        ch = self.channel_base
        self.encoder_block = nn.ModuleList()
        for l, mult in enumerate(channel_mult):
            for _ in range(n_res_blocks):
                self.encoder_block.append(
                    ResidualBlock(ch, mult * self.channel_base, time_channel=time_embedding_channel, dropout=dropout)
                )
                ch = mult * self.channel_base
                channel_sequence.append(ch)
            if l != len(channel_mult) - 1:
                self.encoder_block.append(SampleBlock(sampling_type="down"))
                channel_sequence.append(ch)
        
        # Projection layer for reference features
        self.ref_proj = nn.Conv2d(self.channel_base * 4, self.channel_features, kernel_size=1) # projects a vector of 256 to 64
        ref_ch = self.channel_features  # Here, 64

        # Bottleneck blocks
        self.bottom_block0 = ResidualBlock(ch, ch, time_channel=time_embedding_channel, dropout=dropout)
        self.cross_attn_block = CrossAttentionBlock(dim=ch, cross_dim=ref_ch, num_heads=attention_head)
        self.bottom_block2 = ResidualBlock(ch, ch, time_channel=time_embedding_channel, dropout=dropout)
        
        # Decoder blocks
        self.decoder_block = nn.ModuleList()
        for l, mult in reversed(list(enumerate(channel_mult))):
            for _ in range(n_res_blocks):
                self.decoder_block.append(
                    ResidualBlock(ch + channel_sequence.pop(), mult * self.channel_base,
                                  time_channel=time_embedding_channel, dropout=dropout, cbam=cbam)
                )
                ch = mult * self.channel_base
            if l > 0:
                self.decoder_block.append(
                    Sequential(
                        ResidualBlock(ch + channel_sequence.pop(), mult * self.channel_base, time_channel=time_embedding_channel, dropout=dropout),
                        SampleBlock(sampling_type="up")
                    )
                )
                ch = mult * self.channel_base
        
        # Output layer
        self.output = nn.Sequential(
            nn.GroupNorm(32, ch),
            nn.SiLU(),
            nn.Conv2d(ch, self.channel_out, kernel_size=1)
        )
        
    def forward(self, x, t=None, ref_feats=None):
        """
        Forward pass of the network.

        Args:
        - x (torch.Tensor): Input tensor of shape [batch_size, channels, height, width].
        - t (torch.Tensor, optional): Time embedding tensor. Default is None.
        - ref_feats (torch.Tensor, optional): Reference features for cross-attention. Default is None.

        Returns:
        - torch.Tensor: Output tensor after applying the denoising UNet.
        """
        # Compute time embedding
        t_emb = self.time_embedding(torch.randn(x.shape[0], self.channel_base).to(x.device)) if t is not None else None
        h = self.input(x)
        ht = [h]
        for module in self.encoder_block:
            h = module(h, t_emb)
            ht.append(h)
        h = self.bottom_block0(h, t_emb)
        if ref_feats is not None:
            # Project reference features to match the number of channels
            ref_feats = self.ref_proj(ref_feats)
            if ref_feats.shape[-2:] != h.shape[-2:]:
                ref_feats = F.interpolate(ref_feats, size=h.shape[-2:], mode="nearest")
            h = self.cross_attn_block(h, ref_feats)
            h = self.bottom_block2(h, t_emb)   # To remove ?
        for module in self.decoder_block:
            h = torch.cat([h, ht.pop()], dim=1)
            h = module(h, t_emb)
        return self.output(h)

----
models/components/xt_module.py
"""
===========================================================
Extended Module for Time-Dependent Layers
===========================================================

This script defines an extended module class for handling time-dependent layers in neural networks. It includes the following classes:
1. Module: Abstract base class for time-dependent layers.
2. Sequential: Sequential container for stacking time-dependent layers.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

from abc import abstractmethod
import torch.nn as nn

class Module(nn.Module):
    """
    Abstract base class for time-dependent layers.

    Args:
        x (torch.Tensor): Input tensor.
        t (torch.Tensor): Time step tensor.

    Returns:
        torch.Tensor: Output tensor after applying the layer.
    """
    @abstractmethod
    def forward(self, x, t):
        raise NotImplemented

class Sequential(nn.Sequential, Module):
    """
    Sequential container for stacking time-dependent layers.

    Args:
        x (torch.Tensor): Input tensor.
        t (torch.Tensor): Time step tensor.

    Returns:
        torch.Tensor: Output tensor after applying all layers in sequence.
    """
    @abstractmethod
    def forward(self, x, t):
        for layer in self:
            x = layer(x, t) if isinstance(layer, Module) else layer(x)
        return x
----
models/components/time_embedding.py
"""
===========================================================
Time Embedding Utility
===========================================================

This script contains a utility function for generating sinusoidal time embeddings. It includes the following function:
1. time_embedding: Generate a sinusoidal time embedding for a given time step.

Author: Axel Delaval and Adama Koïta
Year: 2025
===========================================================
"""

import torch
import math

def time_embedding(time_step, dimension, max_period=1000):
    """
    Generate a sinusoidal time embedding for a given time step.

    Args:
        time_step (torch.Tensor): Tensor of shape [N], one per batch element.
        dimension (int): The dimension of the output embedding.
        max_period (int): Maximum period for the sinusoidal embedding.

    Returns:
        torch.Tensor: Tensor of shape [N, dimension] containing the time embeddings.
    """
    half = dimension // 2
    freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=time_step.device)
    args = time_step[:, None].float() * freqs[None]
    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)
    if dimension % 2:
        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)
    return embedding
----
psc_project/utils/meters.py
import torch


class AverageMeter(object):
    """Computes and stores the average and current value"""

    def __init__(self, name, fmt=':f', category="train"):
        self.name = name
        self.category = category
        self.fmt = fmt
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count

    def log(self, writer, n):
        writer.add_scalar(self.category + "/" + self.name, self.val, n)

    def __str__(self):
        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'
        return fmtstr.format(**self.__dict__)


class ProgressMeter(object):
    def __init__(self, num_batches, meters, prefix=""):
        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)
        self.meters = meters
        self.prefix = prefix

    def display(self, batch):
        entries = [self.prefix + self.batch_fmtstr.format(batch)]
        entries += [str(meter) for meter in self.meters]
        print('\t'.join(entries))

    def _get_batch_fmtstr(self, num_batches):
        num_digits = len(str(num_batches // 1))
        fmt = '{:' + str(num_digits) + 'd}'
        return '[' + fmt + '/' + fmt.format(num_batches) + ']'


def accuracy(output, target, topk=(1,)):
    """Computes the accuracy over the k top predictions for the specified values of k"""
    with torch.no_grad():
        maxk = max(topk)
        batch_size = target.size(0)

        _, pred = output.topk(maxk, 1, True, True)
        pred = pred.t()
        correct = pred.eq(target.view(1, -1).expand_as(pred))

        res = []
        for k in topk:
            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)
            res.append(correct_k.mul_(100.0 / batch_size))
        return res
----
psc_project/utils/__init__.py
from utils.losses import MaskMSE
from utils.meters import AverageMeter, ProgressMeter, accuracy
from utils.pck import proj_kps, compute_pck
from utils.spatial_transforms import *
from utils.compute_flow import compute_flow
----
psc_project/utils/losses.py
import torch
import torch.nn as nn
import torch.nn.functional as F


class MaskMSE(nn.Module):
    """
    A masked version of MSE, designed for flow.
    It only cares about valid pixels, and ignores errors at pixels with out-of-bound values.
    Ignore conditions: value == 0: typically caused by zero padding in grid_sample
                       value > 1.0 or value < -1.0: typically happened when flow indices went out of range

    """
    def __init__(self):
        super().__init__()

    def forward(self, input, target):
        mse = F.mse_loss(input, target, reduction="none")

        mask = torch.ones(mse.shape, dtype=torch.bool).to(mse.device)
        input_mask = input != 0
        target_mask = target != 0
        mask = mask & input_mask & target_mask

        input_mask = torch.abs(input) <= 1
        target_mask = torch.abs(target) <= 1
        mask = mask & input_mask & target_mask
        mask = mask.detach().flatten()

        mse = mse.flatten()
        return mse[mask].mean()

----
psc_project/utils/spatial_transforms.py
""" Extracted from NC-Net. """
from __future__ import print_function, division
import numpy as np
import random
import torch
import torch.nn.functional as F

from torch.nn.modules.module import Module


def expand_dim(tensor, dim, desired_dim_len):
    sz = list(tensor.size())
    sz[dim] = desired_dim_len
    return tensor.expand(tuple(sz))


def mask_outlier(flow):
    mask = (flow < -1.0) & (flow > 1.0)
    flow[mask] = 1e4
    return flow


def flow_grid_sample(baseflow, flow, mode, padding_mode, align_corners):
    out = F.grid_sample(baseflow.permute(0, 3, 1, 2), flow, mode=mode, padding_mode=padding_mode, align_corners=align_corners).permute(0, 2, 3, 1)
    return out


class AffineGridGen(Module):
    """Dense correspondence map generator, corresponding to an affine transform."""
    def __init__(self, out_h=240, out_w=240, out_ch=3, use_cuda=True):
        super(AffineGridGen, self).__init__()
        self.out_h = out_h
        self.out_w = out_w
        self.out_ch = out_ch

    def forward(self, theta):
        b = theta.size()[0]
        if not theta.size() == (b, 2, 3):
            theta = theta.view(-1, 2, 3)
        theta = theta.contiguous()
        batch_size = theta.size()[0]
        out_size = torch.Size((batch_size, self.out_ch, self.out_h, self.out_w))
        return F.affine_grid(theta, out_size)


class AffineGridGenV2(Module):
    """Dense correspondence map generator, corresponding to an affine  transform."""
    def __init__(self, out_h=240, out_w=240, use_cuda=True):
        super(AffineGridGenV2, self).__init__()
        self.out_h, self.out_w = out_h, out_w
        self.use_cuda = use_cuda

        # create grid in numpy
        # self.grid = np.zeros( [self.out_h, self.out_w, 3], dtype=np.float32)
        # sampling grid with dim-0 coords (Y)
        self.grid_X, self.grid_Y = np.meshgrid(np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))
        # grid_X,grid_Y: load_size [1,H,W,1,1]
        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)
        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)
        self.grid_X.requires_grad = False
        self.grid_Y.requires_grad = False
        if use_cuda:
            self.grid_X = self.grid_X.cuda()
            self.grid_Y = self.grid_Y.cuda()

    def forward(self, theta):
        b = theta.size(0)
        if not theta.size() == (b, 6):
            theta = theta.view(b, 6)
            theta = theta.contiguous()

        t0 = theta[:, 0].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t1 = theta[:, 1].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t2 = theta[:, 2].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t3 = theta[:, 3].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t4 = theta[:, 4].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        t5 = theta[:, 5].unsqueeze(1).unsqueeze(2).unsqueeze(3)

        grid_X = expand_dim(self.grid_X, 0, b)
        grid_Y = expand_dim(self.grid_Y, 0, b)
        grid_Xp = grid_X * t0 + grid_Y * t1 + t2
        grid_Yp = grid_X * t3 + grid_Y * t4 + t5

        return torch.cat((grid_Xp, grid_Yp), 3)


class HomographyGridGen(Module):
    """Dense correspondence map generator, corresponding to a homography transform."""
    def __init__(self, out_h=240, out_w=240, use_cuda=True):
        super(HomographyGridGen, self).__init__()
        self.out_h, self.out_w = out_h, out_w
        self.use_cuda = use_cuda

        # create grid in numpy
        # self.grid = np.zeros( [self.out_h, self.out_w, 3], dtype=np.float32)
        # sampling grid with dim-0 coords (Y)
        self.grid_X, self.grid_Y = np.meshgrid(np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))
        # grid_X,grid_Y: load_size [1,H,W,1,1]
        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)
        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)
        self.grid_X.requires_grad = False
        self.grid_Y.requires_grad = False
        if use_cuda:
            self.grid_X = self.grid_X.cuda()
            self.grid_Y = self.grid_Y.cuda()

    def forward(self, theta):
        b = theta.size(0)
        if theta.size(1) == 9:
            H = theta
        else:
            H = homography_mat_from_4_pts(theta)
        h0 = H[:, 0].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h1 = H[:, 1].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h2 = H[:, 2].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h3 = H[:, 3].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h4 = H[:, 4].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h5 = H[:, 5].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h6 = H[:, 6].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h7 = H[:, 7].unsqueeze(1).unsqueeze(2).unsqueeze(3)
        h8 = H[:, 8].unsqueeze(1).unsqueeze(2).unsqueeze(3)

        grid_X = expand_dim(self.grid_X, 0, b)
        grid_Y = expand_dim(self.grid_Y, 0, b)

        grid_Xp = grid_X * h0 + grid_Y * h1 + h2
        grid_Yp = grid_X * h3 + grid_Y * h4 + h5
        k = grid_X * h6 + grid_Y * h7 + h8

        grid_Xp /= k
        grid_Yp /= k

        return torch.cat((grid_Xp, grid_Yp), 3)


def homography_mat_from_4_pts(theta):
    b = theta.size(0)
    if not theta.size() == (b, 8):
        theta = theta.view(b, 8)
        theta = theta.contiguous()

    xp = theta[:, :4].unsqueeze(2);
    yp = theta[:, 4:].unsqueeze(2)

    x = torch.FloatTensor([-1, -1, 1, 1]).unsqueeze(1).unsqueeze(0).expand(b, 4, 1)
    y = torch.FloatTensor([-1, 1, -1, 1]).unsqueeze(1).unsqueeze(0).expand(b, 4, 1)
    z = torch.zeros(4).unsqueeze(1).unsqueeze(0).expand(b, 4, 1)
    o = torch.ones(4).unsqueeze(1).unsqueeze(0).expand(b, 4, 1)
    single_o = torch.ones(1).unsqueeze(1).unsqueeze(0).expand(b, 1, 1)

    if theta.is_cuda:
        x = x.cuda()
        y = y.cuda()
        z = z.cuda()
        o = o.cuda()
        single_o = single_o.cuda()

    A = torch.cat([torch.cat([-x, -y, -o, z, z, z, x * xp, y * xp, xp], 2),
                   torch.cat([z, z, z, -x, -y, -o, x * yp, y * yp, yp], 2)], 1)
    # find homography by assuming h33 = 1 and inverting the linear system
    h = torch.bmm(torch.inverse(A[:, :, :8]), -A[:, :, 8].unsqueeze(2))
    # add h33
    h = torch.cat([h, single_o], 1)

    H = h.squeeze(2)

    return H


class TpsGridGen(Module):
    """Dense correspondence map generator, corresponding to a TPS transform.
    https://github.com/cheind/py-thin-plate-spline/blob/master/thinplate/pytorch.py"""
    def __init__(self, out_h=240, out_w=240, use_regular_grid=True, grid_size=3, reg_factor=0, use_cuda=True):
        super(TpsGridGen, self).__init__()
        self.out_h, self.out_w = out_h, out_w
        self.reg_factor = reg_factor
        self.use_cuda = use_cuda

        # create grid in numpy
        # self.grid = np.zeros( [self.out_h, self.out_w, 3], dtype=np.float32)
        # sampling grid with dim-0 coords (Y)
        self.grid_X, self.grid_Y = np.meshgrid(np.linspace(-1, 1, out_w), np.linspace(-1, 1, out_h))
        # grid_X,grid_Y: load_size [1,H,W,1,1]
        self.grid_X = torch.FloatTensor(self.grid_X).unsqueeze(0).unsqueeze(3)
        self.grid_Y = torch.FloatTensor(self.grid_Y).unsqueeze(0).unsqueeze(3)
        self.grid_X.requires_grad = False
        self.grid_Y.requires_grad = False
        if use_cuda:
            self.grid_X = self.grid_X.cuda()
            self.grid_Y = self.grid_Y.cuda()

        # initialize regular grid for control points P_i
        if use_regular_grid:
            axis_coords = np.linspace(-1, 1, grid_size)
            self.N = grid_size * grid_size
            P_Y, P_X = np.meshgrid(axis_coords, axis_coords)
            P_X = np.reshape(P_X, (-1, 1))  # load_size (N,1)
            P_Y = np.reshape(P_Y, (-1, 1))  # load_size (N,1)
            P_X = torch.FloatTensor(P_X)
            P_Y = torch.FloatTensor(P_Y)
            self.Li = self.compute_L_inverse(P_X, P_Y).unsqueeze(0)
            self.P_X = P_X.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0, 4)
            self.P_Y = P_Y.unsqueeze(2).unsqueeze(3).unsqueeze(4).transpose(0, 4)
            self.P_X.requires_grad = False
            self.P_Y.requires_grad = False
            if use_cuda:
                self.P_X = self.P_X.cuda()
                self.P_Y = self.P_Y.cuda()

    def forward(self, theta):
        warped_grid = self.apply_transformation(theta, torch.cat((self.grid_X, self.grid_Y), 3))

        return warped_grid

    def compute_L_inverse(self, X, Y):
        N = X.size()[0]  # num of points (along dim 0)
        # construct matrix K
        Xmat = X.expand(N, N)
        Ymat = Y.expand(N, N)
        P_dist_squared = torch.pow(Xmat - Xmat.transpose(0, 1), 2) + torch.pow(Ymat - Ymat.transpose(0, 1), 2)
        P_dist_squared[P_dist_squared == 0] = 1  # make diagonal 1 to avoid NaN in log computation
        K = torch.mul(P_dist_squared, torch.log(P_dist_squared))
        if self.reg_factor != 0:
            K += torch.eye(K.size(0), K.size(1)) * self.reg_factor
        # construct matrix L
        O = torch.FloatTensor(N, 1).fill_(1)
        Z = torch.FloatTensor(3, 3).fill_(0)
        P = torch.cat((O, X, Y), 1)
        L = torch.cat((torch.cat((K, P), 1), torch.cat((P.transpose(0, 1), Z), 1)), 0)
        Li = torch.inverse(L)
        if self.use_cuda:
            Li = Li.cuda()
        return Li

    def apply_transformation(self, theta, points):
        if theta.dim() == 2:
            theta = theta.unsqueeze(2).unsqueeze(3)
        # points should be in the [B,H,W,2] format,
        # where points[:,:,:,0] are the X coords
        # and points[:,:,:,1] are the Y coords

        # input are the corresponding control points P_i
        batch_size = theta.size()[0]
        # split theta into point coordinates
        Q_X = theta[:, :self.N, :, :].squeeze(3)
        Q_Y = theta[:, self.N:, :, :].squeeze(3)

        # get spatial dimensions of points
        points_b = points.size()[0]
        points_h = points.size()[1]
        points_w = points.size()[2]

        # repeat pre-defined control points along spatial dimensions of points to be transformed
        P_X = self.P_X.expand((1, points_h, points_w, 1, self.N))
        P_Y = self.P_Y.expand((1, points_h, points_w, 1, self.N))

        # compute weigths for non-linear part
        W_X = torch.bmm(self.Li[:, :self.N, :self.N].expand((batch_size, self.N, self.N)), Q_X)
        W_Y = torch.bmm(self.Li[:, :self.N, :self.N].expand((batch_size, self.N, self.N)), Q_Y)
        # reshape
        # W_X,W,Y: load_size [B,H,W,1,N]
        W_X = W_X.unsqueeze(3).unsqueeze(4).transpose(1, 4).repeat(1, points_h, points_w, 1, 1)
        W_Y = W_Y.unsqueeze(3).unsqueeze(4).transpose(1, 4).repeat(1, points_h, points_w, 1, 1)
        # compute weights for affine part
        A_X = torch.bmm(self.Li[:, self.N:, :self.N].expand((batch_size, 3, self.N)), Q_X)
        A_Y = torch.bmm(self.Li[:, self.N:, :self.N].expand((batch_size, 3, self.N)), Q_Y)
        # reshape
        # A_X,A,Y: load_size [B,H,W,1,3]
        A_X = A_X.unsqueeze(3).unsqueeze(4).transpose(1, 4).repeat(1, points_h, points_w, 1, 1)
        A_Y = A_Y.unsqueeze(3).unsqueeze(4).transpose(1, 4).repeat(1, points_h, points_w, 1, 1)

        # compute distance P_i - (grid_X,grid_Y)
        # grid is expanded in point dim 4, but not in batch dim 0, as points P_X,P_Y are fixed for all batch
        points_X_for_summation = points[:, :, :, 0].unsqueeze(3).unsqueeze(4).expand(
            points[:, :, :, 0].size() + (1, self.N))
        points_Y_for_summation = points[:, :, :, 1].unsqueeze(3).unsqueeze(4).expand(
            points[:, :, :, 1].size() + (1, self.N))

        if points_b == 1:
            delta_X = points_X_for_summation - P_X
            delta_Y = points_Y_for_summation - P_Y
        else:
            # use expanded P_X,P_Y in batch dimension
            delta_X = points_X_for_summation - P_X.expand_as(points_X_for_summation)
            delta_Y = points_Y_for_summation - P_Y.expand_as(points_Y_for_summation)

        dist_squared = torch.pow(delta_X, 2) + torch.pow(delta_Y, 2)
        # U: load_size [1,H,W,1,N]
        dist_squared[dist_squared == 0] = 1  # avoid NaN in log computation
        U = torch.mul(dist_squared, torch.log(dist_squared))

        # expand grid in batch dimension if necessary
        points_X_batch = points[:, :, :, 0].unsqueeze(3)
        points_Y_batch = points[:, :, :, 1].unsqueeze(3)
        if points_b == 1:
            points_X_batch = points_X_batch.expand((batch_size,) + points_X_batch.size()[1:])
            points_Y_batch = points_Y_batch.expand((batch_size,) + points_Y_batch.size()[1:])

        points_X_prime = A_X[:, :, :, :, 0] + \
                         torch.mul(A_X[:, :, :, :, 1], points_X_batch) + \
                         torch.mul(A_X[:, :, :, :, 2], points_Y_batch) + \
                         torch.sum(torch.mul(W_X, U.expand_as(W_X)), 4)

        points_Y_prime = A_Y[:, :, :, :, 0] + \
                         torch.mul(A_Y[:, :, :, :, 1], points_X_batch) + \
                         torch.mul(A_Y[:, :, :, :, 2], points_Y_batch) + \
                         torch.sum(torch.mul(W_Y, U.expand_as(W_Y)), 4)

        return torch.cat((points_X_prime, points_Y_prime), 3)


class ComposedGeometricTnf(object):
    """
    Composed geometric transformation (affine+tps)
    """

    def __init__(self, tps_grid_size=3, tps_reg_factor=0, out_h=240, out_w=240,
                 offset_factor=1.0,
                 padding_crop_factor=None,
                 use_cuda=True):
        self.padding_crop_factor = padding_crop_factor

        self.affTnf = GeometricTnf(out_h=out_h, out_w=out_w,
                                   geometric_model='affine',
                                   offset_factor=offset_factor if padding_crop_factor is None else padding_crop_factor,
                                   use_cuda=use_cuda)

        self.tpsTnf = GeometricTnf(out_h=out_h, out_w=out_w,
                                   geometric_model='tps',
                                   tps_grid_size=tps_grid_size,
                                   tps_reg_factor=tps_reg_factor,
                                   offset_factor=offset_factor if padding_crop_factor is None else 1.0,
                                   use_cuda=use_cuda)

    def __call__(self, theta_aff, theta_aff_tps, use_cuda=True):
        sampling_grid_aff = self.affTnf(image_batch=None,
                                        theta_batch=theta_aff.view(-1, 2, 3),
                                        return_sampling_grid=True,
                                        return_warped_image=False)

        sampling_grid_aff_tps = self.tpsTnf(image_batch=None,
                                            theta_batch=theta_aff_tps,
                                            return_sampling_grid=True,
                                            return_warped_image=False)

        if self.padding_crop_factor is not None:
            sampling_grid_aff_tps = sampling_grid_aff_tps * self.padding_crop_factor

        # put 1e10 value in region out of bounds of sampling_grid_aff

        # compose transformations
        sampling_grid_aff_tps_comp = flow_grid_sample(sampling_grid_aff, sampling_grid_aff_tps,
                                                      padding_mode="border", align_corners=True, mode="bilinear")

        return sampling_grid_aff_tps_comp


class GeometricTnf(object):
    """
    Geometric transformation to an image batch (wrapped in a PyTorch tensor)
    ( can be used with no transformation to perform bilinear resizing )
    """

    def __init__(self, geometric_model='affine', tps_grid_size=3, tps_reg_factor=0, out_h=240, out_w=240,
                 offset_factor=None, use_cuda=True):
        self.out_h = out_h
        self.out_w = out_w
        self.geometric_model = geometric_model
        self.use_cuda = use_cuda
        self.offset_factor = offset_factor

        if geometric_model == 'affine' and offset_factor is None:
            self.gridGen = AffineGridGen(out_h=out_h, out_w=out_w, use_cuda=use_cuda)
        elif geometric_model == 'affine' and offset_factor is not None:
            self.gridGen = AffineGridGenV2(out_h=out_h, out_w=out_w, use_cuda=use_cuda)
        elif geometric_model == 'hom':
            self.gridGen = HomographyGridGen(out_h=out_h, out_w=out_w, use_cuda=use_cuda)
        elif geometric_model == 'tps':
            self.gridGen = TpsGridGen(out_h=out_h, out_w=out_w, grid_size=tps_grid_size,
                                      reg_factor=tps_reg_factor, use_cuda=use_cuda)
        if offset_factor is not None:
            self.gridGen.grid_X = self.gridGen.grid_X / offset_factor
            self.gridGen.grid_Y = self.gridGen.grid_Y / offset_factor

        self.theta_identity = torch.Tensor(np.expand_dims(np.array([[1, 0, 0], [0, 1, 0]]), 0).astype(np.float32))
        if use_cuda:
            self.theta_identity = self.theta_identity.cuda()

    def __call__(self, image_batch, theta_batch=None, out_h=None, out_w=None, return_warped_image=True,
                 return_sampling_grid=False, padding_factor=1.0, crop_factor=1.0):
        if image_batch is None:
            b = 1
        else:
            b = image_batch.size(0)
        if theta_batch is None:
            theta_batch = self.theta_identity
            theta_batch = theta_batch.expand(b, 2, 3).contiguous()
            theta_batch.requires_grad=False

            # check if output dimensions have been specified at call time and have changed
        if (out_h is not None and out_w is not None) and (out_h != self.out_h or out_w != self.out_w):
            if self.geometric_model == 'affine':
                gridGen = AffineGridGen(out_h, out_w, use_cuda=self.use_cuda)
            elif self.geometric_model == 'hom':
                gridGen = HomographyGridGen(out_h, out_w, use_cuda=self.use_cuda)
            elif self.geometric_model == 'tps':
                gridGen = TpsGridGen(out_h, out_w, use_cuda=self.use_cuda)
        else:
            gridGen = self.gridGen

        sampling_grid = gridGen(theta_batch)

        # rescale grid according to crop_factor and padding_factor
        if padding_factor != 1 or crop_factor != 1:
            sampling_grid = sampling_grid * (padding_factor * crop_factor)
        # rescale grid according to offset_factor
        if self.offset_factor is not None:
            sampling_grid = sampling_grid * self.offset_factor
        sampling_grid = mask_outlier(sampling_grid)

        if return_sampling_grid and not return_warped_image:
            return sampling_grid

        # sample transformed image
        warped_image_batch = F.grid_sample(image_batch, sampling_grid, align_corners=True)

        if return_sampling_grid and return_warped_image:
            return (warped_image_batch, sampling_grid)

        return warped_image_batch


class SynthecticAffHomoTPSTransfo:
    """Generates a flow field of given load_size, corresponding to a randomly sampled Affine, Homography, TPS or Affine-TPS
    transformation. """
    def __init__(self, size_output_flow=(480, 640), random_t=0.25, random_s=(0.5, 1.5), random_alpha=np.pi / 12,
                 random_t_tps_for_afftps=None, random_t_hom=0.4, random_t_tps=0.4, tps_grid_size=3, tps_reg_factor=0,
                 flip=False, transformation_types=None, parametrize_with_gaussian=False, use_cuda=True):
        """
        For all transformation parameters, image is taken as in interval [-1, 1]. Therefore all parameters must be
        within [0, 1]. The range of sampling is then [-parameter, parameter] or [1-parameter, 1+parameter] for the
        scale.
        Args:
            size_output_flow: desired output load_size for generated flow field
            random_t: max translation for affine transform.
            random_s: max scale for affine transform
            random_alpha: max rotation and shearing angle for the affine transform
            random_t_tps_for_afftps: max translation parameter for the tps transform generation, used for the
                         affine-tps transforms
            random_t_hom: max translation parameter for the homography transform generation
            random_t_tps: max translation parameter for the tps transform generation
            tps_grid_size: tps grid load_size
            tps_reg_factor:
            transformation_types: list of transformations to samples.
                                  Must be selected from ['affine', 'hom', 'tps', 'afftps']
            parametrize_with_gaussian: sampling distribution for the transformation parameters. Gaussian ? otherwise,
                                       uses a uniform distribution
            use_cuda: use_cuda?
        """

        if not isinstance(size_output_flow, tuple):
            size_output_flow = (size_output_flow, size_output_flow)
        self.out_h, self.out_w = size_output_flow
        self.parametrize_with_gaussian = parametrize_with_gaussian
        # for homo
        self.random_t_hom = random_t_hom
        # for tps
        self.random_t_tps = random_t_tps
        # for affine
        self.random_t = random_t
        self.random_alpha = random_alpha
        self.random_s = random_s
        self.tps_grid_size = tps_grid_size
        if random_t_tps_for_afftps is None:
            random_t_tps_for_afftps = random_t_tps
        self.random_t_tps_for_afftps = random_t_tps_for_afftps
        self.use_cuda = use_cuda
        self.flip = flip
        if transformation_types is None:
            transformation_types = ['affine', 'hom', 'tps', 'afftps']
        self.transformation_types = transformation_types
        if 'hom' in self.transformation_types:
            self.homo_grid_sample = HomographyGridGen(out_h=self.out_h, out_w=self.out_w, use_cuda=use_cuda)
        if 'affine' in self.transformation_types:
            self.aff_grid_sample = AffineGridGen(out_h=self.out_h, out_w=self.out_w, use_cuda=use_cuda)
        # self.aff_grid_sample_2 = AffineGridGenV2(out_h=self.out_h, out_w=self.out_w, use_cuda=use_cuda) for offset
        if 'tps' in self.transformation_types:
            self.tps_grid_sample = TpsGridGen(out_h=self.out_h, out_w=self.out_w, grid_size=tps_grid_size,
                                              reg_factor=tps_reg_factor, use_cuda=use_cuda)
        if 'afftps' in self.transformation_types:
            self.tps_aff_grid_sample = ComposedGeometricTnf(tps_grid_size=tps_grid_size, tps_reg_factor=tps_reg_factor,
                                                            out_h=self.out_h, out_w=self.out_w, offset_factor=1.0,
                                                            padding_crop_factor=None, use_cuda=use_cuda)

    def __call__(self, *args, **kwargs):
        """Generates a flow_field (flow_gt) from sampling a geometric transformation. """

        geometric_model = self.transformation_types[random.randrange(0, len(self.transformation_types))]
        # sample the theta
        theta_tps, theta_hom, theta_aff = 0.0, 0.0, 0.0
        if self.parametrize_with_gaussian:
            if geometric_model == 'affine' or geometric_model == 'afftps':
                rot_angle = np.random.normal(0, self.random_alpha, 1)
                sh_angle = np.random.normal(0, self.random_alpha, 1)

                # use uniform, because gaussian distribution is unbounded
                lambda_1 = np.random.uniform(self.random_s[0], self.random_s[1],
                                             1)  # between 0.75 and 1.25 for random_s = 0.25
                lambda_2 = np.random.uniform(self.random_s[0], self.random_s[1], 1)  # between 0.75 and 1.25
                tx = np.random.normal(0, self.random_t, 1)
                ty = np.random.normal(0, self.random_t, 1)

                R_sh = np.array([[np.cos(sh_angle[0]), -np.sin(sh_angle[0])],
                                 [np.sin(sh_angle[0]), np.cos(sh_angle[0])]])
                R_alpha = np.array([[np.cos(rot_angle[0]), -np.sin(rot_angle[0])],
                                    [np.sin(rot_angle[0]), np.cos(rot_angle[0])]])

                D = np.diag([lambda_1[0], lambda_2[0]])

                A = R_alpha @ R_sh.transpose() @ D @ R_sh

                theta_aff = np.array([A[0, 0], A[0, 1], tx[0], A[1, 0], A[1, 1], ty[0]])
                theta_aff = torch.Tensor(theta_aff.astype(np.float32)).unsqueeze(0)
                theta_aff = theta_aff.cuda() if self.use_cuda else theta_aff
            if geometric_model == 'hom':
                theta_hom = np.array([-1, -1, 1, 1, -1, 1, -1, 1])
                theta_hom = theta_hom + np.random.normal(0, self.random_t_hom, 8)
                theta_hom = torch.Tensor(theta_hom.astype(np.float32)).unsqueeze(0)
                theta_hom = theta_hom.cuda() if self.use_cuda else theta_hom
            if geometric_model == 'tps':
                x = np.linspace(-1.0, 1.0, self.tps_grid_size)
                y = np.linspace(-1.0, 1.0, self.tps_grid_size)
                X, Y = np.meshgrid(x, y)
                theta_tps = np.concatenate([Y, X]).flatten()
                theta_tps = theta_tps + np.random.normal(0, self.random_t_tps, self.tps_grid_size * self.tps_grid_size * 2)
                theta_tps = torch.Tensor(theta_tps.astype(np.float32)).unsqueeze(0)
                theta_tps = theta_tps.cuda() if self.use_cuda else theta_tps
            if geometric_model == 'afftps':
                x = np.linspace(-1.0, 1.0, self.tps_grid_size)
                y = np.linspace(-1.0, 1.0, self.tps_grid_size)
                X, Y = np.meshgrid(x, y)
                theta_tps = np.concatenate([Y, X]).flatten()
                theta_tps = theta_tps + np.random.normal(0, self.random_t_tps_for_afftps, self.tps_grid_size * self.tps_grid_size * 2)
                theta_tps = torch.Tensor(theta_tps.astype(np.float32)).unsqueeze(0)
                theta_tps = theta_tps.cuda() if self.use_cuda else theta_tps
        else:
            if geometric_model == 'affine' or geometric_model == 'afftps':
                rot_angle = (np.random.rand(1) - 0.5) * 2 * self.random_alpha
                # between -np.pi/12 and np.pi/12 for random_alpha = np.pi/12
                sh_angle = (np.random.rand(1) - 0.5) * 2 * self.random_alpha
                lambda_1 = np.random.uniform(self.random_s[0], self.random_s[1],
                                             1)  # between 0.75 and 1.25 for random_s = 0.25
                lambda_2 = np.random.uniform(self.random_s[0], self.random_s[1], 1)  # between 0.75 and 1.25
                tx = (2 * np.random.rand(1) - 1) * self.random_t  # between -0.25 and 0.25 for random_t=0.25
                ty = (2 * np.random.rand(1) - 1) * self.random_t


                R_sh = np.array([[np.cos(sh_angle[0]), -np.sin(sh_angle[0])],
                                 [np.sin(sh_angle[0]), np.cos(sh_angle[0])]])
                R_alpha = np.array([[np.cos(rot_angle[0]), -np.sin(rot_angle[0])],
                                    [np.sin(rot_angle[0]), np.cos(rot_angle[0])]])

                D = np.diag([lambda_1[0], lambda_2[0]])

                A = R_alpha @ R_sh.transpose() @ D @ R_sh

                theta_aff = np.array([A[0, 0], A[0, 1], tx[0], A[1, 0], A[1, 1], ty[0]])
                theta_aff = torch.Tensor(theta_aff.astype(np.float32)).unsqueeze(0)
                theta_aff = theta_aff.cuda() if self.use_cuda else theta_aff
            if geometric_model == 'hom':
                theta_hom = np.array([-1, -1, 1, 1, -1, 1, -1, 1])
                theta_hom = theta_hom + (np.random.rand(8) - 0.5) * 2 * self.random_t_hom
                theta_hom = torch.Tensor(theta_hom.astype(np.float32)).unsqueeze(0)
                theta_hom = theta_hom.cuda() if self.use_cuda else theta_hom

            if geometric_model == 'tps':
                x = np.linspace(-1.0, 1.0, self.tps_grid_size)
                y = np.linspace(-1.0, 1.0, self.tps_grid_size)
                X, Y = np.meshgrid(x, y)
                theta_tps = np.concatenate([Y, X]).flatten()
                theta_tps = theta_tps + (np.random.rand(self.tps_grid_size * self.tps_grid_size * 2) - 0.5) * 2 * self.random_t_tps
                theta_tps = torch.Tensor(theta_tps.astype(np.float32)).unsqueeze(0)
                theta_tps = theta_tps.cuda() if self.use_cuda else theta_tps
            if geometric_model == 'afftps':
                x = np.linspace(-1.0, 1.0, self.tps_grid_size)
                y = np.linspace(-1.0, 1.0, self.tps_grid_size)
                X, Y = np.meshgrid(x, y)
                theta_tps = np.concatenate([Y, X]).flatten()
                theta_tps = theta_tps + (np.random.rand(self.tps_grid_size * self.tps_grid_size * 2) - 0.5) * 2 * self.random_t_tps_for_afftps
                theta_tps = torch.Tensor(theta_tps.astype(np.float32)).unsqueeze(0)
                theta_tps = theta_tps.cuda() if self.use_cuda else theta_tps

        if geometric_model == 'hom':
            flow_gt = self.homo_grid_sample.forward(theta_hom)
            # flow_gt = unormalise_and_convert_mapping_to_flow(mapping, output_channel_first=True)  # should be 2xhw
        elif geometric_model == 'affine':
            flow_gt = self.aff_grid_sample.forward(theta_aff)
            # flow_gt = unormalise_and_convert_mapping_to_flow(mapping, output_channel_first=True)  # should be 2xhw

        elif geometric_model == 'tps':
            flow_gt = self.tps_grid_sample.forward(theta_tps)
            # flow_gt = unormalise_and_convert_mapping_to_flow(mapping, output_channel_first=True)  # should be 2xhw

        elif geometric_model == 'afftps':
            flow_gt = self.tps_aff_grid_sample(theta_aff, theta_tps)
            # flow_gt = unormalise_and_convert_mapping_to_flow(mapping, output_channel_first=True)  # should be 2xhw

        else:
            raise NotImplementedError

        if self.flip:
            if torch.rand(1) < 0.5:
                flow_gt = torch.flip(flow_gt, dims=[2])

        return flow_gt


class FlowComposition:
    def __init__(self, list_of_gens):
        self.gens = list_of_gens

    def __call__(self, *args, **kwargs):
        flow = self.gens[0]()
        for i in range(1, len(self.gens)):
            flow2 = self.gens[i]()
            flow = flow_grid_sample(flow, flow2, mode="bilinear", padding_mode="border", align_corners=True)
        return flow




----
psc_project/utils/pck.py
import torch
import numpy as np


def proj_kps(flows, kps, image_size):
    """
    Project src keypoints to the dst image using estimated flows.
    :param flows: Shape: N, H, W, 2
    :param kps: Shape: N, 8, 2
    :param image_size: Size of image. Default: 256.
    :return: The projected keypoints.
    """
    N, H, W, _ = flows.shape
    flows = (flows + 1) * (image_size - 1) / 2
    flows = flows.reshape(N, H * W, 2)
    kps = torch.round(kps).long()
    kps = kps.reshape(N, 8, 2)
    kps = kps[:, :, 0] * image_size + kps[:, :, 1]
    kps = kps.unsqueeze(-1).expand(-1, -1, 2)
    dst = torch.gather(flows, 1, kps).flip(dims=(-1,))
    return dst


def compute_pck(gt_kps, pred_kps, image_size):
    """
    Compute PCK@15, PCK@0.10, PCK@0.05
    :param gt_kps: groundtruth keypoints annotation
    :param pred_kps: predicted keypoints
    :param image_size: size of image
    :return: PCK@10, PCK@5
    """
    diff = np.sqrt(np.sum(np.square(np.array(gt_kps) - np.array(pred_kps)), axis=-1))  # N, 8
    pck10 = np.sum(diff <= image_size * 0.1, axis=1) / 8
    pck05 = np.sum(diff <= image_size * 0.05, axis=1) / 8
    return pck10, pck05

----
psc_project/models/PSCNet.py
import torch
import torch.nn as nn
import torch.nn.functional as F


def mask_outlier(flow):
    """
    Mask the out-of-bound flow with 1e4
    :param flow: the displacement field
    :return: the masked displacement field
    """
    mask = (flow < -1.0) & (flow > 1.0)
    flow[mask] = 1e4
    return flow


def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1, bias: bool = False) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=bias, dilation=dilation)

def conv1x1(in_planes: int, out_planes: int, groups: int = 1, bias=False) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, groups=groups, bias=bias)


class STN(nn.Module):
    """
    The Spatial Transformer Network (STN) that computes correlation between feature maps
    and estimates displacement field.
    """
    def __init__(self, corr_layer, feat_size, stn_size, stn_layer):
        """
        :param corr_layer: (list of ints) The feature layer(s) that we compute the correlations over.
                           Normally set to [3] or [2, 3].
        :param feat_size: (int) Size of the feature map. Default: 16.
        :param stn_size: (int) Size of the predicted displacement field. Default: 16.
        :param stn_layer: (int) Number of STN layers. Default: 5.
        """
        super(STN, self).__init__()
        self.corr_layer = corr_layer
        self.feat_size = feat_size
        self.stn_size = stn_size

        # build STN blocks at the scale of 4x4, 8x8, and 16x16
        self.net_4x = self._build_block(4, stn_layer)
        self.net_8x = self._build_block(8, stn_layer)
        self.net_16x = self._build_block(16, stn_layer)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.normal_(m.weight, std=1e-3)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # generate an identity displacement field
        self.register_buffer("base_map",
                             F.affine_grid(torch.Tensor([[1, 0, 0], [0, 1, 0]]).unsqueeze(0),
                                           [1, 1, self.stn_size, self.stn_size], align_corners=True))

    def _build_block(self, output_size, n_layers):
        ds = self.feat_size // output_size
        assert ds in [1, 2, 4, 8, 16]
        dims = self.feat_size * self.feat_size * len(self.corr_layer)

        layers = [
            nn.Conv2d(dims, dims, ds, stride=ds),
            nn.BatchNorm2d(dims),
            nn.LeakyReLU(0.1),
        ]

        for i in range(n_layers):
            layers += [
                conv3x3(dims, dims // 2),
                nn.BatchNorm2d(dims // 2),
                nn.LeakyReLU(0.1)
            ]
            dims = dims // 2

        layers += [
            conv3x3(dims, 2, bias=True),
            nn.Upsample(size=(self.stn_size, self.stn_size), mode="bilinear", align_corners=True)
        ]

        return nn.Sequential(*layers)

    def corr(self, map1, map2):
        '''
        Computes the correlation matrix from the N selected feature layers
        :param map1: Dict of feature maps from source image
        :param map2: Dict of feature maps from target image
        :return: The concatenated correlation matrix (B, W * H * N, W, H)
        '''
        corrs = []

        map1 = self.interpolate(map1, (self.feat_size, self.feat_size))
        map2 = self.interpolate(map2, (self.feat_size, self.feat_size))

        for i in self.corr_layer:
            map1_ = map1["layer%i" % i]
            map2_ = map2["layer%i" % i]
            N, C, W, H = map1_.shape
            map1_ = map1_.permute(0, 2, 3, 1).contiguous().view(N, W * H, C)
            map2_ = map2_.view(N, C, W * H)
            corr = torch.matmul(map1_, map2_).view(N, W * H, W, H)
            corrs.append(corr)
        corrs = torch.cat(corrs, dim=1)

        return corrs

    def interpolate(self, map1, size):
        """
        Interpolate each feature layer to the output size.
        :param map1: Dict of feature maps.
        :param size: (tuple of ints) Output size of feature map.
        :return:
        """
        map1_ = {}
        for i in self.corr_layer:
            map1_["layer%i" % i] = F.interpolate(map1["layer%i" % i], size, mode="bilinear", align_corners=False)
        return map1_

    def grid_sample(self, map1, flow):
        """
        Warp the feature maps using the flow (displacement field).
        :param map1: Dict of feature maps.
        :param flow: Displacement field.
        :return:
        """
        map1_ = {}
        for i in self.corr_layer:
            map1_["layer%i" % i] = F.grid_sample(map1["layer%i" % i], flow,
                                                 mode="bilinear", padding_mode="zeros", align_corners=True)
        return map1_

    def forward(self, map1, map2, training=False):

        # computes displacement field at scale 4x4
        corr_4x = self.corr(map2, map1)
        # init with the identity displacement field
        flow_4x = self.net_4x(corr_4x).permute(0, 2, 3, 1) + self.base_map
        flow_4x = mask_outlier(flow_4x)

        # computes displacement field at scale 8x8
        # warp source feature maps with the 4x4 displacement field, so the block only predicts the residue.
        map1_8x = self.grid_sample(map1, flow_4x)
        corr_8x = self.corr(map2, map1_8x)
        flow_8x = self.net_8x(corr_8x).permute(0, 2, 3, 1)
        flow_8x = flow_4x + flow_8x
        flow_8x = mask_outlier(flow_8x)

        # computes displacement field at scale 16x16
        # warp source feature maps with the 8x8 displacement field, so the block only predicts the residue.
        map1_16x = self.grid_sample(map1, flow_8x)
        corr_16x = self.corr(map2, map1_16x)
        flow_16x = self.net_16x(corr_16x).permute(0, 2, 3, 1)
        flow_16x = flow_8x + flow_16x
        flow_16x = mask_outlier(flow_16x)

        # return multi-scale flows during training.
        if training:
            return flow_16x, flow_8x, flow_4x
        else:
            return flow_16x


class PSCNet(nn.Module):
    """
    The overall network with feature encoder and warp estimator.
    """
    def __init__(self, framework, backbone, dim, corr_layer, feat_size=16, stn_size=16,
                 pretrained_encoder="", stn_layer=5,
                 replace_stride_with_dilation=[False, False, False], **kwargs):
        """

        :param framework: Feature encoder training framework. Default: MoCo v2.
        :param backbone: Feature encoder backbone. Default: ResNet w/ conditional BN.
        :param dim: (int) MoCo dim. Default: 128.
        :param corr_layer: (list of ints) The feature layer(s) that we compute the correlations over.
                           Normally set to [3] or [2, 3].
        :param feat_size: (int) Size of the feature map. Default: 16.
        :param stn_size: (int) Size of the predicted displacement field. Default: 16.
        :param stn_layer: (int) Number of STN layers. Default: 5.
        :param pretrained_encoder: (str) Path to a pretrained encoder weights. Default: "".
        :param replace_stride_with_dilation: (list of bools) Whether to replace the stride with dilation
                                             in ResNet block 2, 3, 4. Used for larger feature maps.
        :param kwargs: Other parameters to the MoCo framework.
        """
        super().__init__()

        self.corr_layer = corr_layer
        self.feat_size = feat_size
        self.stn_size = stn_size

        self.encoder_q = backbone(num_classes=dim, pretrained=pretrained_encoder,
                                  replace_stride_with_dilation=replace_stride_with_dilation)
        self.encoder_k = backbone(num_classes=dim, pretrained=pretrained_encoder,
                                  replace_stride_with_dilation=replace_stride_with_dilation)

        self.encoder_q.fc = nn.Identity()
        self.encoder_k.fc = nn.Identity()

        self.framework = framework(self.encoder_q, self.encoder_k, dim, **kwargs)

        self.stn = STN(corr_layer, feat_size, stn_size, stn_layer)

        self.register_buffer("pos_map", F.affine_grid(torch.Tensor([[1, 0, 0], [0, 1, 0]]).unsqueeze(0),
                                                      [1, 1, stn_size, stn_size], align_corners=True).permute(0, 3, 1, 2))

    def forward_framework(self, im_q, im_k, cond_q, cond_k):
        """
        Forward the MoCo framework.
        :param im_q: Query images.
        :param im_k: Key images.
        :param cond_q: BN condition for query images (photo or sketch).
        :param cond_k: BN condition for key images (photo or sketch).
        :return: Logits, target, query feature maps, key feature maps (See MoCo framework for details)
        """
        output, target, res1, res2 = self.framework(im_q=im_q, im_k=im_k,
                                                    cond_q=cond_q, cond_k=cond_k,
                                                    return_map=True)

        return output, target, res1, res2

    def forward_backbone(self, im, cond, corr_only=False):
        """
        Forward the feature encoder backbone.
        :param im: Images.
        :param cond: Condition of BN (photo or sketch).
        :param corr_only: Return feature maps up to the wanted layers only. Used for faster computation.
        :return: FC features, feature maps.
        """
        fc, res = self.framework.encoder_q(im, cond, return_map=True,
                                           corr_layer=self.corr_layer if corr_only else None)

        return fc, res

    def forward_stn(self, map1, map2, dense_mtx=False):
        """
        Forward the STN.
        :param map1: Source feature maps.
        :param map2: Target feature maps.
        :param dense_mtx: (bool) Return dense correspondence matrix.
                          (Only for visualization. We compute error metric with a more accurate method.)
        :return: Forward flow, backward flow, and (optionally) dense correspondence matrix.
        """

        fwd_flow = self.stn(map1, map2)
        bwd_flow = self.stn(map2, map1)

        if dense_mtx:
            dist = self.stoch_dist([fwd_flow])
            return fwd_flow, bwd_flow, dist
        else:
            return fwd_flow, bwd_flow

    def stoch_dist(self, fwd_flows):
        """
        Find the dense correspondence matrix from displacement field.
        Only for visualization. We compute error metric with a more accurate method.
        :param fwd_flows: Forward displacement field.
        :return: The dense correspondence matrix.
        """
        N = fwd_flows[0].shape[0]
        pos_map = self.pos_map.repeat(N, 1, 1, 1)
        fwd_map = F.grid_sample(pos_map, fwd_flows[0], padding_mode="border", align_corners=True)

        for fwd_flow in fwd_flows[1:]:
            fwd_map = F.grid_sample(fwd_map, fwd_flow, padding_mode="border", align_corners=True)

        dist = torch.cdist(pos_map.permute(0, 2, 3, 1).view(N, -1, 2), fwd_map.permute(0, 2, 3, 1).view(N, -1, 2))
        return dist

    def compute_similarity(self, map1, map2):
        """
        Compute similarity and weight map at the selected feature layers.
        :param map1: Dict of feature maps.
        :param map2: Dict of feature maps.
        :return: The similarity and weight map.
        """
        map1_list = []
        map2_list = []
        for i in self.corr_layer:
            map1_list.append(F.interpolate(map1["layer%i" % i], (self.stn_size, self.stn_size), mode="bilinear"))
            map2_list.append(F.interpolate(map2["layer%i" % i], (self.stn_size, self.stn_size), mode="bilinear"))
        map1 = torch.cat(map1_list, dim=1)
        map2 = torch.cat(map2_list, dim=1)

        map1 = F.interpolate(map1, (self.stn_size, self.stn_size), mode="bilinear")
        map2 = F.interpolate(map2, (self.stn_size, self.stn_size), mode="bilinear")

        # compute similarity and weight map
        N, C, W, H = map1.shape
        D = W * H

        map1 = map1.permute(0, 2, 3, 1).contiguous().view(N, D, C)
        map2 = map2.view(N, C, D)

        corr = torch.matmul(map1, map2).view(N, D, D)
        corr = F.normalize(corr, dim=1, p=1)
        corr = F.normalize(corr, dim=2, p=1)
        weight = torch.max(corr, dim=2)[0]
        weight = weight - weight.min()
        weight = weight / weight.max()
        return [corr], [weight.detach()]

----
psc_project/models/resnet_cbn.py
import torch
from torch import Tensor
import torch.nn as nn
from typing import Type, Any, Callable, Union, List, Optional, Tuple
from torch.hub import load_state_dict_from_url

__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152']

model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',
    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',
    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',
    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',
    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',
}

PADDING = "replicate"


def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation, padding_mode=PADDING)


def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class CondBatchNorm2d(nn.Module):
    def __init__(self, num_features, num_conds=2):
        super().__init__()
        self.norms = nn.ModuleList([nn.BatchNorm2d(num_features)] * num_conds)

    def forward(self, x, cond):
        return self.norms[cond](x)


class Sequential(nn.Sequential):
    def __init__(self, *args):
        super().__init__(*args)

    def forward(self, *args):
        x = args[0]
        cond = args[1]
        for module in self._modules.values():
            if isinstance(module, (BasicBlock, Bottleneck, Sequential, CondBatchNorm2d, ResNet)):
                x = module(x, cond)
            else:
                x = module(x)
        return x


class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
            self,
            inplanes: int,
            planes: int,
            stride: int = 1,
            downsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes, dilation=dilation)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor, cond: int) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out, cond)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out, cond)

        if self.downsample is not None:
            identity = self.downsample(x, cond)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion: int = 4

    def __init__(
            self,
            inplanes: int,
            planes: int,
            stride: int = 1,
            downsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor, cond: int) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out, cond)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out, cond)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out, cond)

        if self.downsample is not None:
            identity = self.downsample(x, cond)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(
            self,
            block: Type[Union[BasicBlock, Bottleneck]],
            layers: List[int],
            num_classes: int = 1000,
            zero_init_residual: bool = False,
            groups: int = 1,
            width_per_group: int = 64,
            replace_stride_with_dilation: Optional[List[bool]] = None,
            norm_layer: Optional[Callable[..., nn.Module]] = None,
            ignore_cond: bool = False
    ) -> None:
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = CondBatchNorm2d
        self._norm_layer = norm_layer
        self.ignore_cond = ignore_cond

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError("replace_stride_with_dilation should be None "
                             "or a 3-element tuple, got {}".format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,
                               bias=False, padding_mode=PADDING)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
                                       dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
                                       dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
                                       dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.norms[0].weight, 0)  # type: ignore[arg-type]
                    nn.init.constant_(m.bn3.norms[1].weight, 0)
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.norms[0].weight, 0)  # type: ignore[arg-type]
                    nn.init.constant_(m.bn2.norms[1].weight, 0)

    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,
                    stride: int = 1, dilate: bool = False) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,
                            self.base_width, previous_dilation, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=self.groups,
                                base_width=self.base_width, dilation=self.dilation,
                                norm_layer=norm_layer))

        return Sequential(*layers)

    def _forward_impl(self, x: Tensor, cond: int, return_map: bool = True, corr_layer=None):
        # See note [TorchScript super()]
        y = dict()

        x = self.conv1(x)
        x = self.bn1(x, cond)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x, cond)
        y["layer1"] = x

        x = self.layer2(x, cond)
        y["layer2"] = x

        x = self.layer3(x, cond)
        y["layer3"] = x

        if corr_layer is None:
            x = self.layer4(x, cond)
            y["layer4"] = x
        else:
            if 4 in corr_layer:
                x = self.layer4(x, cond)
                y["layer4"] = x

        if corr_layer is None:
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)

            if return_map:
                return x, y
            else:
                return x
        else:
            return None, y

    def forward(self, x: Tensor, cond=None, return_map=False, corr_layer=None):
        if self.ignore_cond:
            cond = 0
        else:
            if cond is None:
                raise ValueError("missing parameter \'cond\'")
        return self._forward_impl(x, cond, return_map, corr_layer)


def _resnet(
        arch: str,
        block: Type[Union[BasicBlock, Bottleneck]],
        layers: List[int],
        pretrained: str,
        progress: bool,
        **kwargs: Any
) -> ResNet:
    model = ResNet(block, layers, **kwargs)
    if pretrained != "":
        if pretrained == "sup":
            state_dict = load_state_dict_from_url(model_urls[arch],
                                                  progress=progress)
        else:
            state_dict = torch.load(pretrained)['state_dict']
        for k in list(state_dict.keys()):
            if "fc" in k:
                state_dict.pop(k)
            if "bn" in k or "downsample.1" in k:
                bn0 = k.split(".")
                bn1 = k.split(".")
                bn0.insert(len(bn0) - 1, "norms.0")
                bn1.insert(len(bn1) - 1, "norms.1")
                bn0 = ".".join(bn0)
                bn1 = ".".join(bn1)
                state_dict[bn0] = state_dict[k]
                state_dict[bn1] = state_dict[k]
                state_dict.pop(k)
        msg = model.load_state_dict(state_dict, strict=False)
        assert set(msg.missing_keys) == {"fc.weight", "fc.bias"}
    return model


def resnet18(pretrained: str = "", progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-18 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)


def resnet34(pretrained: str = "", progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-34 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet50(pretrained: str = "", progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-50 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet101(pretrained: str = "", progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-101 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)


def resnet152(pretrained: str = "", progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-152 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)
----
psc_project/models/resnet_orig.py
import torch
from torch import Tensor
import torch.nn as nn
from torch.hub import load_state_dict_from_url
from typing import Type, Any, Callable, Union, List, Optional

__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',
           'resnet152', 'resnext50_32x4d', 'resnext101_32x8d',
           'wide_resnet50_2', 'wide_resnet101_2']

model_urls = {
    'resnet18': 'https://download.pytorch.org/models/resnet18-f37072fd.pth',
    'resnet34': 'https://download.pytorch.org/models/resnet34-b627a593.pth',
    'resnet50': 'https://download.pytorch.org/models/resnet50-0676ba61.pth',
    'resnet101': 'https://download.pytorch.org/models/resnet101-63fe2227.pth',
    'resnet152': 'https://download.pytorch.org/models/resnet152-394f9c45.pth',
    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',
    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',
    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',
    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',
}

PADDING_MODE = "replicate"


def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=dilation, groups=groups, bias=False, dilation=dilation, padding_mode=PADDING_MODE)


def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:
    """1x1 convolution"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)


class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
            self,
            inplanes: int,
            planes: int,
            stride: int = 1,
            downsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(BasicBlock, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError('BasicBlock only supports groups=1 and base_width=64')
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride, dilation=dilation)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes, dilation=dilation)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class Bottleneck(nn.Module):
    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)
    # while original implementation places the stride at the first 1x1 convolution(self.conv1)
    # according to "Deep residual learning for image recognition"https://arxiv.org/abs/1512.03385.
    # This variant is also known as ResNet V1.5 and improves accuracy according to
    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.

    expansion: int = 4

    def __init__(
            self,
            inplanes: int,
            planes: int,
            stride: int = 1,
            downsample: Optional[nn.Module] = None,
            groups: int = 1,
            base_width: int = 64,
            dilation: int = 1,
            norm_layer: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(Bottleneck, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        width = int(planes * (base_width / 64.)) * groups
        # Both self.conv2 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv1x1(inplanes, width)
        self.bn1 = norm_layer(width)
        self.conv2 = conv3x3(width, width, stride, groups, dilation)
        self.bn2 = norm_layer(width)
        self.conv3 = conv1x1(width, planes * self.expansion)
        self.bn3 = norm_layer(planes * self.expansion)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)
        out = self.relu(out)

        out = self.conv3(out)
        out = self.bn3(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out += identity
        out = self.relu(out)

        return out


class ResNet(nn.Module):

    def __init__(
            self,
            block: Type[Union[BasicBlock, Bottleneck]],
            layers: List[int],
            num_classes: int = 1000,
            zero_init_residual: bool = False,
            groups: int = 1,
            width_per_group: int = 64,
            replace_stride_with_dilation: Optional[List[bool]] = None,
            norm_layer: Optional[Callable[..., nn.Module]] = None
    ) -> None:
        super(ResNet, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        self._norm_layer = norm_layer

        self.inplanes = 64
        self.dilation = 1
        if replace_stride_with_dilation is None:
            # each element in the tuple indicates if we should replace
            # the 2x2 stride with a dilated convolution instead
            replace_stride_with_dilation = [False, False, False]
        if len(replace_stride_with_dilation) != 3:
            raise ValueError("replace_stride_with_dilation should be None "
                             "or a 3-element tuple, got {}".format(replace_stride_with_dilation))
        self.groups = groups
        self.base_width = width_per_group
        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,
                               bias=False, padding_mode=PADDING_MODE)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,
                                       dilate=replace_stride_with_dilation[0])
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,
                                       dilate=replace_stride_with_dilation[1])
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,
                                       dilate=replace_stride_with_dilation[2])
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)

        # Zero-initialize the last BN in each residual branch,
        # so that the residual branch starts with zeros, and each residual block behaves like an identity.
        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677
        if zero_init_residual:
            for m in self.modules():
                if isinstance(m, Bottleneck):
                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]
                elif isinstance(m, BasicBlock):
                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]

    def _make_layer(self, block: Type[Union[BasicBlock, Bottleneck]], planes: int, blocks: int,
                    stride: int = 1, dilate: bool = False) -> nn.Sequential:
        norm_layer = self._norm_layer
        downsample = None
        previous_dilation = self.dilation
        if dilate:
            self.dilation *= stride
            stride = 1
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                conv1x1(self.inplanes, planes * block.expansion, stride),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,
                            self.base_width, previous_dilation, norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, groups=self.groups,
                                base_width=self.base_width, dilation=self.dilation,
                                norm_layer=norm_layer))

        return nn.Sequential(*layers)

    def _forward_impl(self, x: Tensor, cond: int, return_map: bool = False) -> Tensor:
        # See note [TorchScript super()]
        y = dict()
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        y["layer1"] = x
        x = self.layer2(x)
        y["layer2"] = x
        x = self.layer3(x)
        y["layer3"] = x
        x = self.layer4(x)
        y["layer4"] = x

        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)

        if return_map:
            return x, y
        return x

    def forward(self, x: Tensor, cond: int, return_map: bool = False) -> Tensor:
        return self._forward_impl(x, cond, return_map)


def _resnet(
        arch: str,
        block: Type[Union[BasicBlock, Bottleneck]],
        layers: List[int],
        pretrained: bool,
        progress: bool,
        **kwargs: Any
) -> ResNet:
    model = ResNet(block, layers, **kwargs)
    if pretrained:
        state_dict = load_state_dict_from_url(model_urls[arch],
                                              progress=progress)
        for k in list(state_dict.keys()):
            if "fc" in k:
                state_dict.pop(k)
        model.load_state_dict(state_dict, strict=False)
    return model


def resnet18(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-18 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,
                   **kwargs)


def resnet34(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-34 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet34', BasicBlock, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet50(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-50 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet50', Bottleneck, [3, 4, 6, 3], pretrained, progress,
                   **kwargs)


def resnet101(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-101 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,
                   **kwargs)


def resnet152(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNet-152 model from
    `"Deep Residual Learning for Image Recognition" <https://arxiv.org/pdf/1512.03385.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    return _resnet('resnet152', Bottleneck, [3, 8, 36, 3], pretrained, progress,
                   **kwargs)


def resnext50_32x4d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNeXt-50 32x4d model from
    `"Aggregated Residual Transformation for Deep Neural Networks" <https://arxiv.org/pdf/1611.05431.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    kwargs['groups'] = 32
    kwargs['width_per_group'] = 4
    return _resnet('resnext50_32x4d', Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def resnext101_32x8d(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""ResNeXt-101 32x8d model from
    `"Aggregated Residual Transformation for Deep Neural Networks" <https://arxiv.org/pdf/1611.05431.pdf>`_.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    kwargs['groups'] = 32
    kwargs['width_per_group'] = 8
    return _resnet('resnext101_32x8d', Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)


def wide_resnet50_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""Wide ResNet-50-2 model from
    `"Wide Residual Networks" <https://arxiv.org/pdf/1605.07146.pdf>`_.

    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    kwargs['width_per_group'] = 64 * 2
    return _resnet('wide_resnet50_2', Bottleneck, [3, 4, 6, 3],
                   pretrained, progress, **kwargs)


def wide_resnet101_2(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> ResNet:
    r"""Wide ResNet-101-2 model from
    `"Wide Residual Networks" <https://arxiv.org/pdf/1605.07146.pdf>`_.

    The model is the same as ResNet except for the bottleneck number of channels
    which is twice larger in every block. The number of channels in outer 1x1
    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048
    channels, and in Wide ResNet-50-2 has 2048-1024-2048.

    Args:
        pretrained (bool): If True, returns a model pre-trained on ImageNet
        progress (bool): If True, displays a progress bar of the download to stderr
    """
    kwargs['width_per_group'] = 64 * 2
    return _resnet('wide_resnet101_2', Bottleneck, [3, 4, 23, 3],
                   pretrained, progress, **kwargs)

----
psc_project/models/moco.py
# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved
import torch
import torch.nn as nn


class MoCo(nn.Module):
    """
    Build a MoCo model with: a query encoder, a key encoder, and a queue
    https://arxiv.org/abs/1911.05722
    """
    def __init__(self, encoder_q, encoder_k, dim=128, K=65536, m=0.999, T=0.07):
        """
        dim: feature dimension (default: 128)
        K: queue size; number of negative keys (default: 65536)
        m: moco momentum of updating key encoder (default: 0.999)
        T: softmax temperature (default: 0.07)
        """
        super(MoCo, self).__init__()

        self.K = K
        self.m = m
        self.T = T

        # create the encoders
        # num_classes is the output fc dimension

        self.encoder_q = encoder_q
        self.encoder_k = encoder_k
        dim_mlp = self.encoder_q.layer4[-1].conv1.weight.shape[1]

        self.fc_q = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),
                                  nn.ReLU(),
                                  nn.Linear(dim_mlp, dim))
        self.fc_k = nn.Sequential(nn.Linear(dim_mlp, dim_mlp),
                                  nn.ReLU(),
                                  nn.Linear(dim_mlp, dim))

        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data.copy_(param_q.data)  # initialize
            param_k.requires_grad = False  # not update by gradient

        for param_q, param_k in zip(self.fc_q.parameters(), self.fc_k.parameters()):
            param_k.data.copy_(param_q.data)  # initialize
            param_k.requires_grad = False  # not update by gradient

        # create the queue
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = nn.functional.normalize(self.queue, dim=0)

        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))

    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        """
        Momentum update of the key encoder
        """
        for param_q, param_k in zip(self.encoder_q.parameters(), self.encoder_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)
        for param_q, param_k in zip(self.fc_q.parameters(), self.fc_k.parameters()):
            param_k.data = param_k.data * self.m + param_q.data * (1. - self.m)

    @torch.no_grad()
    def _dequeue_and_enqueue(self, keys):
        # gather keys before updating queue
        keys = concat_all_gather(keys)

        batch_size = keys.shape[0]

        ptr = int(self.queue_ptr)
        assert self.K % batch_size == 0  # for simplicity

        # replace the keys at ptr (dequeue and enqueue)
        self.queue[:, ptr:ptr + batch_size] = keys.T
        ptr = (ptr + batch_size) % self.K  # move pointer

        self.queue_ptr[0] = ptr

    @torch.no_grad()
    def _batch_shuffle_ddp(self, x):
        """
        Batch shuffle, for making use of BatchNorm.
        *** Only support DistributedDataParallel (DDP) model. ***
        """
        # gather from all gpus
        batch_size_this = x.shape[0]
        x_gather = concat_all_gather(x)
        batch_size_all = x_gather.shape[0]

        num_gpus = batch_size_all // batch_size_this

        # random shuffle index
        idx_shuffle = torch.randperm(batch_size_all).cuda()

        # broadcast to all gpus
        torch.distributed.broadcast(idx_shuffle, src=0)

        # index for restoring
        idx_unshuffle = torch.argsort(idx_shuffle)

        # shuffled index for this gpu
        gpu_idx = torch.distributed.get_rank()
        idx_this = idx_shuffle.view(num_gpus, -1)[gpu_idx]

        return x_gather[idx_this], idx_unshuffle

    @torch.no_grad()
    def _batch_unshuffle_ddp(self, x, idx_unshuffle):
        """
        Undo batch shuffle.
        *** Only support DistributedDataParallel (DDP) model. ***
        """
        # gather from all gpus
        batch_size_this = x.shape[0]
        x_gather = concat_all_gather(x)
        batch_size_all = x_gather.shape[0]

        num_gpus = batch_size_all // batch_size_this

        # restored index for this gpu
        gpu_idx = torch.distributed.get_rank()
        idx_this = idx_unshuffle.view(num_gpus, -1)[gpu_idx]

        return x_gather[idx_this]

    def forward(self, im_q, im_k, cond_q, cond_k, return_map=False):
        """
        Input:
            im_q: a batch of query images
            im_k: a batch of key images
        Output:
            logits, targets
        """

        # compute query features
        if return_map:
            q, q_res = self.encoder_q(im_q, cond_q, return_map=True)  # queries: NxC
            q = self.fc_q(q)
            with torch.no_grad():
                _, k_res = self.encoder_q(im_k, cond_k, return_map=True)  # queries: NxC
        else:
            q = self.encoder_q(im_q, cond_q, return_map=False)
            q = self.fc_q(q)

        q = nn.functional.normalize(q, dim=1)

        # compute key features
        with torch.no_grad():  # no gradient to keys
            self._momentum_update_key_encoder()  # update the key encoder

            # shuffle for making use of BN
            im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
            k = self.encoder_k(im_k, cond_k, return_map=False)
            k = self.fc_k(k)

            k = nn.functional.normalize(k, dim=1)
            k = self._batch_unshuffle_ddp(k, idx_unshuffle)

        # compute logits
        # Einstein sum is more intuitive
        # positive logits: Nx1
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        # negative logits: NxK
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])

        # logits: Nx(1+K)
        logits = torch.cat([l_pos, l_neg], dim=1)

        # apply temperature
        logits /= self.T

        # labels: positive key indicators
        labels = torch.zeros(logits.shape[0], dtype=torch.long).cuda()

        # dequeue and enqueue
        self._dequeue_and_enqueue(k)

        if return_map:
            return logits, labels, q_res, k_res
        else:
            return logits, labels


# utils
@torch.no_grad()
def concat_all_gather(tensor):
    """
    Performs all_gather operation on the provided tensors.
    *** Warning ***: torch.distributed.all_gather has no gradient.
    """
    tensors_gather = [torch.ones_like(tensor)
        for _ in range(torch.distributed.get_world_size())]
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)

    output = torch.cat(tensors_gather, dim=0)
    return output
--END--